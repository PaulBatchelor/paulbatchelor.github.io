<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<link rel="stylesheet" href="/css/style.css">

</head>
<body>
<div id="main">
<title>Overview</title>
<h1>Overview</h1>

<h2>Overview (of the Overview)</h2>

<p>This pages attempts to provide a birds-eye view of what
Monolith is and what it does. Things will start with
a very high-level description, then immediately build up
to this starting from the lowest-level principles covered
within the scope of monolith.
</p>

<h2>The Tippy Top</h2>

<p>Monolith is the name of software that I (Paul) use to
create computer generated sound and music.
</p>
<p>Monolith is a <code>live-coding</code> environment for sound,
typically spawned inside of the <code>Emacs</code> text editor.
In this setup, I write code (a dialect of <code>Scheme</code>), and
this is sent to Monolith to tell it to create sound.
</p>
<p>In addition to code, I also use a number of interfaces with
my setup. Most notably the <code>monome</code> <code>grid</code> and <code>arc</code>, and
sometimes the griffin powermate USB, which I refer to as
the '<code>griffin</code>'. These are tightly integrated with Monolith,
and an important aspect of Monolith's design.
</p>
<p>In some words: music, computers, real-time, live-coding,
interfaces.
</p>

<h2>The Bottom of the Monolith: Soundpipe</h2>

<p>The bottom layer of Monolith is a library called Soundpipe.
</p>
<p>Right before it reaches your speaker and comes out as sound
waves, sound is a bunch of discrete numbers. These numbers
are produced using something known as digital signal
processing, or <code>DSP</code>.
</p>
<p>The thing that handles all DSP is done with something called
the <code>DSP Kernel</code>, which can be examined in detail at
<a href="/proj/monolith/program/wm_000_0221#wm_000_0221">5. DSP Kernel</a>.
</p>
<p>The majority of DSP is handled with Soundpipe, a library
written in portable C code. Soundpipe has a nice
collection of music DSP algorithms that provide monolith
a good starting point.
</p>
<p>Because it's written in C, it's very hard to be creative in
Soundpipe. For every musical thought, several of lines of
C code need to be written. This takes time to do. Too much
time.
</p>
<p>To fix this, a few constructs will be written on top of
Soundpipe. Historically, this was a language called Sporth.
Spiritually, Sporth lives on in two components: <code>Patchwerk</code>and <code>Runt</code>. Soundpipe is managed inside of Patchwerk. You
can read more about this relationship in <a href="/proj/monolith/program/wm_000_0439#wm_000_0439">6. Patchwerk and Soundpipe</a>. Runt runs on
top of Patchwerk, and will be covered later in the document.
</p>

<h2>Making structures with Patchwerk</h2>

<p><code>patchwerk</code> is another C library, used on top of Soundpipe.
It solves one problem: how to connect self-contained DSP
algorithms (like the ones seen in Soundpipe).
</p>
<p>Patchwerk constructs what electronic musicians would call a
<code>patch</code>, and computer scientists would call a directed audio
graph. A patch is made up of small building blocks of sound
called <code>nodes</code>, and nodes talk to one another using virtual
<code>cables</code>. There's a bit more, but that's the gist.
</p>
<p>Patchwerk gives Soundpipe a generalized way to talk to
itself, and with other DSP code converted to patchwerk.
Every Soundpipe DSP module gets wrapped up in a Patchwerk
node. It's very useful, but it's still very tedious to
write Patchwerk C code by hand. Sometimes, it's even more
tedious!
</p>
<p>Luckily, things in Patchwerk are predictably tedious.
It is trivial (and encouraged!) to build things on top of
Patchwerk to make things a bit more manageable.
</p>
<p>To see how the Patchwerk C API is used in monolith, consult
<a href="/proj/monolith/program/wm_000_0439#wm_000_0439">6. Patchwerk and Soundpipe</a>.
</p>
<p>The next layer immediately following Patchwerk is <code>Runt</code>.
</p>

<h2>Higher-level control with Runt</h2>

<p><code>runt</code> is a funny little stack-based language written in C.
It is designed to be very portable, easy to embed,
and easy to extend. The core of the Runt is managed in
<a href="/proj/monolith/program/wm_000_0954#wm_000_0954">9. Runt</a>.
</p>
<p>Like most stack based languages and Forths, Runt maintains
a list of keywords known as <code>words</code> in a <code>dictionary</code>. The
Runt C API allows programs to add new words to the
dictionary at run time. Patchwerk API has a number of words
that it adds to Runt. Every Patchwerk node gets
wrapped around a Runt word and then added to the
dictionary. This happens in <a href="/proj/monolith/program/wm_000_0954#wm_000_0971">9.2.1. The Main Runt Loader</a>.
</p>
<p>Runt allows patches in Patchwerk to be described in a very
terse way. Patchwerk is also quite stack-oriented like Runt,
so the flow really makes sense.
</p>
<p>Now, a digression...
</p>
<p>Up to this point, only DSP code has been discussed. Actually
<b>hearing</b> what this DSP code produces is another matter.
In Monolith, sound needs to be heard in realtime. Also, it
wants to be adjusted in realtime. This is where the core
monolith layer comes in.
</p>
<p>There's also the issue of Runt code itself: Runt is a very
terse language, and after about a page, things get very hard
to keep track of. Also, Runt doesn't have a very great REPL
needed for live coding. Both of these problems get solved
when <a href="/proj/monolith/wiki/runt#runt_scheme">Scheme is used to evaluate Runt code</a>.
</p>
<p>Runt code evaluation is managed in <a href="/proj/monolith/program/wm_000_0954#wm_000_1017">9.3. Runt Evaluation</a>. In Scheme, runt code is evaluated with
<code>monolith:runt-eval</code>.
</p>

<h2>Realtime audio and interfaces with Monolith</h2>

<p>The core monolith application layer is written in C, and is
the heartbeat of the monolith system. It does many things,
but the two biggest things it does are handling real-time
audio and hardware interfaces like the <code>arc</code> and <code>monome</code>(<a href="/proj/monolith/program/wm_000_1925#wm_000_1925">13. Monome Hardware (Arc, Grid) and Libmonome</a>).
</p>
<p>For sound, Monolith uses JACK
(<a href="/proj/monolith/program/wm_000_0581#wm_000_0583">7.1. JACK audio</a>). It creates a JACK client that
connects to an existing server, and then a callback that
sits on top of that. This callback renders audio generated
from a patchwerk patch. it also impelements a thing called
<code>hot-swapping</code> in patchwerk, which makes it possible to
throw out an old patchwerk patch and replace it with a new
one. hot-swapping is the key element that makes live-coding
possible (<a href="/proj/monolith/program/wm_000_0221#wm_000_0270">5.2. Hot Swapping</a>).
</p>
<p>Monolith also implements a <code>hardware listener</code> (<a href="/proj/monolith/program/wm_000_1850#wm_000_1850">12. Hardware Listener</a>) that listens
for events from plugged in interfaces. <code>libmonome</code> is used
to poll events from the arc and grid (<a href="/proj/monolith/program/wm_000_1925#wm_000_2127">13.7. Polling and Processing Monome/Arc Events</a>), and <code>libusb</code> and
a internal fork of <code>libhidapi</code> are used to manage everything
else (Griffin, etc).
</p>
<p>Monome users should note that I'm not using OSC layer,
but the api abstractions underneath that OSC layer. OSC
and liblo caused more troubles than they were worth, and
removing it ended up making things way more responsive
and predictable.
</p>
<p>There's a thin layer between the actual
hardware and monolith. Someday, this may make it easier
build virtual versions of the hardware interfaces. Nothing
much done here yet, but it's there. (<a href="/proj/monolith/program/wm_000_0630#wm_000_0630">8. Virtual Interface Layer</a>).
</p>
<p>Special monolith applications are written for the arc and
grid using what is called a <code>page</code> interface
(<a href="/proj/monolith/program/wm_000_1221#wm_000_1221">11. Pages</a>), which allow
hardware interfaces to be used in different configurations
within in the same patch. When a page is selected, it gains
control of the hardware. Only one page can be selected at a
time. Pages also the ability to save and load state data
using a combination of SQLite and MSGPack blobs (<a href="/proj/monolith/program/wm_002">moncmp.c</a>).
</p>
<p>This conceptually encompasses most of what Monolith is:
a mother hen written C that takes care of all her various
little chicks: realtime audio, hot swapping, pages. The
kernel of it all.
</p>
<p>But, Monolith is not usually directly controlled from C.
Instead, it is controlled using scripting languages like
Scheme, and sometimes Janet.
</p>

<h2>Abstraction and expressiveness with Scheme/Janet</h2>

<p>Monolith is mostly controlled using the Scheme programming
language. Specifically, it is forked from an implementation
known as <code>s9</code> scheme, or s9fes, or scheme 9 from extended
space.
</p>
<p>Monolith commands in C are bound to s9 scheme functions (<a href="/proj/monolith/program/wm_000_1113#wm_000_1115">10.1. Scheme Loader</a>).
The s9 scheme interpreter is most often run as a REPL, which
can be run inside of process controlled by emacs.
</p>
<p>From emacs, blocks of scheme code can be sent to the REPL to
be evaluated and processed.
</p>
<p>Scheme is a great language that lends itself well for
live-coding. It's also a very elegant language for building
abstractions on top of the fundamental monolith API
functions from C.
</p>
<p>Not only does Scheme control the Monolith API, it can
also evaluate runt code as if it were inline code.
In this regard, one can think of Scheme as a very powerful
macro language for Runt. Scheme makes it possible to build
reusable synths and sounds. In fact, virtually all of
the <a href="/proj/monolith/wiki/ugens">core sound unit generators</a> used in
Scheme are just wrappers around Runt code.
</p>
<p>Scheme also enables more complex
patches to be built than if it were written in Runt.
</p>

<h2>Oh, there's also graphics too</h2>

<p>Monolith has a very <a href="/proj/monolith/wiki/graphics">limited graphics interface</a>.
Monolith implements a small framebuffer interface with RGB
color with a maximum 320x200 resolution (<a href="/proj/monolith/program/wm_000_2429#wm_000_2430">18.1. The Graphics Framebuffer</a>). In addition, monolith
also has a few basic drawing operations
(<a href="/proj/monolith/program/wm_027_0141#wm_027_0141">10. Drawing Operations</a>), and some
less basic ones (ex: <a href="/proj/monolith/wiki/dither_1bit">dither_1bit</a>,
<a href="/proj/monolith/wiki/fbm">fbm</a>, <a href="/proj/monolith/program/wm_037">fuzzydot.c</a>).
</p>
<p>Instead of using Scheme, drawing operations use the <code>Janet</code>programming language. (This was done because I wanted to try
out Janet).
</p>
<p>Using <code>libx264</code>, framebuffer data can be encoded into h264
video (<a href="/proj/monolith/program/wm_000_2429#wm_000_2655">18.4. H264 Video Support</a>).
Monolith sound buffers can also be rendered offline
simultaneously. The <code>ffmpeg</code> application is then used to
stitch them up together and create video.
</p>
<p>For <a href="/proj/monolith/wiki/audiovisual">audio-visual works</a>, the Janet
VM gets controlled inside Scheme.
</p>

<h2>In conclusion</h2>

<p>Monolith roughly works out to be:
</p>
<p>music->computers->real-time->live-coding->hardware interfaces.
</p>
<p>The software layers rougly work out to be:
</p>
<p>emacs->scheme->runt->patchwerk->soundpipe->C->JACK.
</p>
</div>
</body>
</html>
