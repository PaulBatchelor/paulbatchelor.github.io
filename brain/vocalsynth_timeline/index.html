<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<link rel="stylesheet" href="/brain/css/style.css">

</head>
<body>
<div id="main">
<title>Vocal Synthesis Timeline</title>
<h1>Vocal Synthesis Timeline</h1>
<p>A very rough timeline of vocal synthesis technologies.
At the moment, just jotting down things from memory.
Obviously not comprehensive by any means.
</p>
<p>References and citations to come, when I have that built
into this wikizet.
</p>
<p>There's a lot more to say on Speech Synthesis than Singing
synthesis. Too much, actually. So, my scope is going to
try to skew towards singing synthesis and musical
applications of speech synthesis.
</p>

<h2>1930's and 1940s: Voder</h2>

<p>In the late 30s and 40s, Bell Labs creates the Voder, an
interface for controlling an electronic voice. The Voder,
despite being synthesized using rudimentary electronical
components, had a surprising range of speech prosody thanks
to the interface. The Voder was notoriously difficult to
control, and very few people were capable of effectively
performing with it. Such is usually the trade-off with
articulative control of artificial voice. It is hard
to have interfaces for artificial voice control with
high ceilings and low floors.
</p>

<h2>1960s: Early Physical Models</h2>

<p>Physically-based computer models for the vocal tract have
been around since the 60s, and singing computers have
existed for almost as long. 1962, John L. Kelly and Carol C.
Lochbaum publish one of the first software implementations
of a physical model of the Vocal Tract. This model was
used the year before to be the singing voice in "Daisy Bell"
by Max Matthews, the first time a computer would be taught
sing, and perhaps one of the earliest significant works of
computer music. This work would go on to influence the
creation of HAL in 2001 a Space Odyssey. HAL would then set
the expectations for what a disembodied computer voice be,
which can still be felt today in todays virtual assistants.
</p>

<h2>70s and 80s</h2>

<p>Cheaper hardware leads to concatenative speech synthesis.
Decline in Spee
</p>
<p>1976: Byte Magazine Volume 00, Number 12: Speech Synthesis
<a href="https://archive.org/details/byte-magazine-1976-08">https://archive.org/details/byte-magazine-1976-08</a></p>
<p>1984: MacSpeak Demo on original macintosh.
</p>

<h2>1991: Perry Cook and Singing Synthesis</h2>

<p>In 1991, Perry Cook publishes a seminal work on articulatory
singing synthesis. In addition to creating novel ways for
analyzing and discovering vocal tract parameters, Cook also
builds an interactive GUI for realtime singing control of
the DSP model. This is perhaps the earliest time realtime
control of such models has been possible, thanks to the
hardware improvements.
</p>

<h2>Early 2000s</h2>


<h3>Vocaloid</h3>

<p>In the early 2000s, a commercial singing synthesizer known
as Vocaloid is born. Under the hood, Vocaloid implements a
proprietary form of concatenative synthesis. Voice sounds
for Vocaloid are created by meticulously sampling the
performances of live singers. Still in development today,
Vocaloid has a rich community and is most definitely
considered "cutting-edge" for singing synthesis in the
industry.
</p>
<p>One of the interesting things about Vocaloid is how they
address the uncanny valley issues that come up when doing
vocal synthesis. Each voice preset, or "performer", is
paired with an cartoon anime performer with a personality
and backstory. Making them cartoons does a lot to steer them
away from the uncanny valley. Unlike most efforts in speech
synthesis, fidelity and even intelligibility are less
important. As a result, Vocaloid has a distinct signature
sound that is both artificial yet familiar.
</p>

<h3>Mullen (2006?): 2d-waveguide of KL model</h3>


<h2>Late 2010s-Present</h2>


<h3>Machine Learning, TTS</h3>

<p>Machine learning techniques used to train data.
Considered to be state of the art for speech synthesis
</p>
<p>Google Wavenet (circa 2016?):
</p>
<p>https://en.wikipedia.org/wiki/WaveNet
</p>
<p>2020s:
</p>
<p>Some singing synthesis using GANS, but not every
interactive.
</p>

<h3>Interactive Web Programs: Pink Trombone, and Blob Opera</h3>

<p>Faster computers means more interactive singing models!
</p>
<p>Relatively recent developments of the web browser have
yielded very interesting interfaces for musical control
of artificial voice. In late 2020, Google releases Blob
Opera, an interactive acapella singing quartet of
anthopomorphic blobs, allegedly using machine learning to
produce chord progressions. A few years earlier, Adult
Swim releases "Choir", a web audio powered quartet with a
similar premise. These are both collaborations by David Li
and Chris Heinrichs. The vocal models here sounds
physically based, but I'm yet to confirm this as the source
code is not available.
</p>
<p>While not deliberately musical like "Blob Opera" or "Choir",
Pink Trombone is a fantastic web app developed by Neil
Thapen, predating the two efforts by a few years. Touted as
low-level speech synthesizer, the Pink Trombone interface
is a anatomic split view of a vocal tract that can be
manipulated in realtime using the mouse or pointer on
mobile. The underlying model is a variation of the
Kelly-Lochbaum physical model, utilizing an analytical
LF glottal model. Pink Trombone serves as the basis of Voc,
a port I made of the DSP layer to ANSI C using a literate
programming style.
</p>
<p>Much of Neil Thapen's work in Pink Trombone can be traced
back to Jack Mullens DSP dissertation on using 2d waveguides
vocal tract control.
</p>

<h3>iOS</h3>

<p>HOWL comes to mind as a pretty decent singing synthesizer
with a pretty novel interface. Circa 2015-2016?
</p>
</div>
</body>
</html>
