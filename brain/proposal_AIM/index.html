<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="/brain/css/style.css">

</head>
<body>
<div id="main">
<title>Research Proposal for AIM program</title>
<h1>Research Proposal for AIM program</h1>

<h2>Initial Things</h2>


<h3>Links</h3>

<p>Application:
<a href="https://www.aim.qmul.ac.uk/apply/">https://www.aim.qmul.ac.uk/apply/</a></p>
<p>PhD topics:
<a href="https://www.aim.qmul.ac.uk/phd-topics/">https://www.aim.qmul.ac.uk/phd-topics/</a><b></b> Relevant Topics (from list)
Relevant Topics for Me (from the PhD list):
</p>
<p>Modelling and Synthesizing Articulation On Acoustic
and Digital Instruments
</p>
<p>Machine Learning of Physical Models
</p>
<p>Multimodal AI for musical collaboration in immersive
environments
</p>
<p>Performance Rendering For Music Generation Systems
</p>

<h3>My own areas and specialization</h3>

<p>Singing Physical Modelling Synthesis
</p>
<p>Audio DSP (for realtime systems)
</p>
<p>Musical Interaction Design
</p>
<p>Computer Music Composition
</p>
<p>Gesture Synthesis
</p>

<h3>Areas To Investigate</h3>

<p>Deep Learning: GANs, Wavenets, etc
</p>
<p>Articulatory Synthesis (more advanced)
</p>
<p>Late Renaissance Counterpoint, Sacred Choral Music
(palestrina, etc)
</p>

<h2>Working Title</h2>

<p>Computer Augmented Techniques in Musical Articulatory Synthesis
</p>

<h2>Outline</h2>


<h3>What is Articulatory Synthesis?</h3>

<p>Particular branch of speech synthesis.
</p>
<p>Attempts to model speech physiologically: synthesize sound
based on physical construction of the vocal apparatus.
</p>
<p>Physically based model breaks down into two components:
glottal airflow (source), tract (filter). Known as
Source-filter model. Glottal source signal goes into tract
filter, and voice-like sounds come out the otherside.
</p>
<p>Human vocal tract is approximated as a series of cylindrical
tubes of varying diameters using a 1-d digital waveguide.
</p>
<p>Different tract shapes creates different formants and vowels.
Account for aerofluidics, and you get frictives and sibilance.
Coordinate everything together, and you get speech.
</p>
<p>Articulatory synthesis is a white-box model. Signals go in,
and vowel formants appear implicitely. Formant synthesis
(also source-filter) is a black box model, using resonators
tuned to create pre-derived formants.
</p>

<h3>The Musical Context</h3>

<p>Singing!
</p>
<p>Singing Synthesis has different priorities than Speech
Synthesis. musical phrasing vs prosody, what makes a
"realistic" performance, emphasis on intelligibility, etc.
</p>

<h3>Ensemble</h3>

<p>Interest in exploring ensemble. 3-6 instances of the model,
and how they can realistically sing together.
</p>

<h3>Why Articulatory Synthesis?</h3>

<p>Why this over other speech synthesis techniques?
It's low fidelity compared to wavenet.
It's also very expensive compared to techniques like
concatenative and formant synthesis.
</p>
<p>So what is there?
</p>
<p>Expressivity. These systems offer a lot of parametric
control over sound output. Granular low level control too.
Much better suited for musical performance. People who
think about shaping and sculpting sound in music. No other
technique is dynamically flexible the way the articulatory
vocal tract model is.
</p>
<p>When articulatory synthesis was popular, computers were not
fast. They are now fast enough to run high-res instances in
realtime.
</p>
<p>It's a far more dynamic system than the existing singing
models out there. With the right training system, it's far
less tedious to create new voices or extend new ones.
</p>

<h3>Computer Augmentation</h3>

<p>Take these very old techniques and try to apply new AI
techniques to them.
</p>

<h3>Areas Of Study</h3>

<p>"singing synthesis" in the context of articulatory synthesis
has many different entry points where new AI can be
introduced to solve problems.
</p>
<p>I have organized the major problems by scale: timbre,
gesture, and ensemble.
</p>

<h4>timbre</h4>

<p>timbre: using AI to faciliate parametric sound control of
the model. Two classic areas of research: finding tract
shapes, and synthesizing glottal flow signals.
</p>
<p>Perhaps a GAN can be used
as a sort of correction filter at the end (style transfer,
etc)?
</p>
<p>Articulatory Inversion: area of research that analyzes voice
and returns corresponding tract shapes.
</p>
<p>Glottal Excitation Extraction: given a real voice, extract
core glottal signal.
</p>
<p>These are common topics in the speech world, but not in
the context of singing.
</p>
<p>For music: exploring vocal
techniques outside of the realm of speech that are utilized
in music. How can we develop models that sound good as a
4-part SATB ensemble?
</p>
<p>"Convincing" as a musical performance
but not necessarily realistic.
</p>
<p>How to develop and dynamically
generate perceptually distinct voices? Reduce the
distinctness to sounding like overdubs?
</p>
<p>This level would be most focused around physical
modelling.
</p>

<h4>gesture</h4>

<p>gesture: the means of articulating and coordinating the
model to be musically coherant. Moving between
vowel/fricative states and pitch, in a musically appropriate
"singing" way.
</p>
<p>Developing "interperation", "attitude", and "personality" in
a musical setting. Lyricism in computer sequenced works.
</p>
<p>Area of ongoing personal research: gesture synthesis.
Building systems that melt sequencing and automation
curves together. My current research explores a novel set of
systems and DSP algorithms that produces audio rate line
signal with timing relative to an external control signal
(a periodic ramp). The lines and curves generated in this
system can also dynamically adapt to tempo flucations in
the external signal in realtime without any preconceived
tempo information.
</p>
<p>This level would be most focused around building smart
articulation and performance systems to control the
underlying singing model.
</p>

<h4>ensemble</h4>

<p>ensemble: coordinating many voices together in a highly
polyphonic setting. dynamic adaptation. things like:
phrasing with context-based-awareness (tempo, micro-timings,
timbre, etc) , and procedurally generated.
</p>
<p>handling one-to-many humans to robots
</p>
<p>creating network ensembles of many humans
performing singing instruments together in V
</p>
<p>a hoarde of robots singing Palestrina.
</p>

<h3>What this isn't</h3>

<p>This isn't speech. Words and intelligibility aren't the
point. The focus is on approaching the voice as a musical
instrument. It's not about the lyrics.
</p>
<p>Fidelity. This is not a focus in so-called "deepfakes" or
voice cloning technology. Analysis techniques utilizing
real voice samples, and even building models that
approximate distinct singers are not out of scope.
Success is measured in musical expressivity. Can we build
singing digital instruments and interfaces from the ground
up that are intuitive and compelling to play.
</p>
<p>This is not NLP or TTS. There is no text to speech. There is
nothing about formal language models. The focus will be on
the sound itself. Not what it is saying. Text to speech is
NOT a musical suitable approach for musical control.
</p>

<h2>Personal Statement</h2>

<p>I teach computers how to sing?
</p>

<h3>Previous Academic Background</h3>

<p>Computer Music At Berklee
</p>
<p>More DSP and programming at Stanford
</p>
<p>I build Musical Software Ecosystems: custom
DSP engines, DSLs, live coding environments,
novel interfaces and digital instruments.
</p>
<p>Design of quirky and unique interfaces that
encourage playfulness and exploration.
</p>
<p>Why the Voice? It's the only time computer music
has a sense of humor.
</p>

<h3>Why this Research?</h3>

<p>As a composer, I have a deep fascination for getting
computers to sing, and to explore the implications of what
that means exactly. I would be very grateful for the
opportunity to pursue this with intense study over a
period of time.
</p>

<h3>Why QMUL?</h3>

<p>I know of QMUL from my networks at CCRMA and Berklee.
</p>
<p>Highly regarded institution for music tech at the
graduate level. Lots of talented people and creative
projects. I see a diverse crowd. Not just STEM people,
and this is very important to me as an
artist-musician-researcher.
</p>

<h2>Working Draft</h2>

<p>See QMUL/prop.txt.
</p>
</div>
</body>
</html>
