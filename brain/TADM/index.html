<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="/brain/css/style.css">

</head>
<body>
<div id="main">
<title>The Algorithm Design Manual</title>
<h1>The Algorithm Design Manual</h1>
<p>By Steven S. Skiena (Third Edition)
</p>
<p>UUID: <code>gipejljdq-eoqa-heio-wauq-idrilewkhhel</code></p>
<p>Recommended book in <a href="/brain/TYCS">TYCS</a>.
</p>
<p>it is divided into two parts, "techniques" and "resources".
The former is a general introduction to algorithm design,
while the latter is inteded for browsing and reference.
</p>

<h2>Chapter 1: Introduction To Algorithm Design</h2>

<p>An algorithm is a procedure to accomplish a specific task.
</p>
<p>An algorithm must solve a generalizable set of problems. An
algorithm will perform on an <code>instance</code> of a problem.
</p>
<p>Examples: Robot Tour Optimization (The Traveling Salesman
problem) and Selecting the Right Jobs.
</p>
<p>Reasonable looking algorithms can be easily incorrect.
Algorithm <code>correctness</code> is a property that must be
carefully demonstrated.
</p>
<p>Proofs are a tool used to distinguish correct algorithms
from incorrect ones. Formal proofs will not be emphasized
in this book.
</p>
<p>Correctness of an algorithm requires that the problem
be carefully and clearly stated. Problem specifications
have two parts: the set of allowed instances, and the
require properties of the algorithm's output.
</p>
<p>An important technique in algorithm design is to narrow
the scope of allowable instances until there is an
efficient solution.
</p>
<p>Avoid ill defined quetions, such as "best" routes (what
does best mean?). Also avoid compound goals.
</p>
<p>Three common forms of algorithmic notation: English,
pseudocode, real programming language.
</p>
<p>Pseudocode: best defined as a programming language that
never complains about syntax errors.
</p>
<p>The heart of an algorithm is an idea. If your idea
is not clearly revealed when you express an algorithm,
then you are using too low-level a notation to describe it.
</p>
<p>Counterexample: an instance for an algorithm which yields
an incorrect answer.
</p>
<p>Counterexample properties: verifiability and simplicity.
</p>
<p>Verifiability: demonstrate that it is a counterexample.
Calculate what the answer would have been, and display
a better answer to prove that the algorithm didn't find it.
</p>
<p>Simplicity: strips away all details, and clearly shows
why the algorithm fails.
</p>
<p>Strategies for finding counterexamples: think small,
think exhaustively, hunt for the weakness, go for a tie,
seek extremes.
</p>
<p>Searching for counterexamples is the best way to
disprove the correctness of a heuristic.
</p>
<p>Failure to find a counterexample to a given algorithm
does it mean "it is obvious" the algorithm is correct.
A proof or demonstration of correctness is needed, often
with the use of mathematical induction.
</p>
<p>Mathematicl induction is usually the right way to
verify the correctness of a recursive or incremental
insertion algorithm.
</p>
<p>Modeling: the art of formulating your application in
terms of precisely described, well understood problems.
</p>
<p>Common structures: permutations, subsets, trees, graphs,
points, polygons, strings.
</p>
<p>Permutations: likely the object in question for problems
that seek "tour, "ordering", or "sequence".
</p>
<p>Subset: likely needed when encountering problems that
seeks a "cluster", "collection", "committee", "group",
"packaging", or "selection".
</p>
<p>Trees: likely object in question when a problem seeks
"hierarchy", "dominance relationship",
"ancestor/descendant relationship", or "taxonomy".
</p>
<p>Graphs: "network", "circuit", "web", or "relationship".
</p>
<p>Points: "sites", "position", "data records", or "locations".
</p>
<p>Polygons: "shapes", "regions", "configurations",
"boundaries".
</p>
<p>Strings: "text", "characters", "patterns", or "labels".
</p>
<p>Modeling your application in terms of well defined
structures and algorithms is the most important single step
towards a solution.
</p>
<p>Thinking recursively: looking for big things that are made
up of smaller things of exactly the same type as the big
thing.
</p>
<p>Proof by Contradiction: assume the hypothesis is false,
develop some logical consequences of assumption, show
that one consequence is false.
</p>
<p>Estimation is principled guessing. Try to solve the problem
in different ways and see if the answers generally agree
in magnitude.
</p>

<h2>Chapter 2: Algorithm Analysis</h2>

<p>Studying algorithm efficiency without implementing them.
</p>
<p>Two tools for analysis: the RAM model of computation and
asymptotic analysis of computational complexity.
</p>
<p>RAM: Random Access Machine. Hypothetical computer machine
where operation time is measured in steps.
</p>
<p>In RAM, the quality of an algorithm is determined by how it
works over all possible instances.
</p>
<p>Best case, worst case, average case.
</p>
<p>Best case: minimum number of steps taken when instance
is size N.
</p>
<p>Worst case: maximum number of steps when instance is size
N.
</p>
<p>Average case: average number of steps when instance is
size N. AKA expected time.
</p>
<p>Worse Case tends to be the most useful in practice.
</p>
<p>Average Case is helpful when working with randomized
algorithms.
</p>
<p>Big-oh notation: simplifies analysis. More "big picture".
</p>
<p>Upper bound, lower bound, something inbetween bound (not
sure what to call this)?
</p>
<p>Big oh ignores difference between multiplicative constants.
</p>
<p>Dominance: faster growing functions dominate slower
growing ones.
</p>
<p>Functions are grouped by different classes, in descending
order: Factorial, Exponential, Cubic, Quadratic,
Superlinear, Linear, Logarithmic, Constant.
</p>
<p>Special considerations for thinking about adding/multipying
functions with Big Oh. (See book).
</p>
<p>Analysis for selection sort (See book for details). Proving
two ways that the Big Oh running time is quadratic (n^2).
</p>
<p>Analysis for insertion sort. Using a "round it up" approach
to find the upper bound worst-case Big Oh time. Useful
approximation for simple algorithm analysis.
</p>
<p>Finding a substring. Analyzes each looping component
of the algorithm, and using Big Oh rules to get
an expression interms of n and m.
</p>
<p>"proving the theta": proving that the time assessment is
correct.
</p>
<p>Matrix Multiplication. Smells like cubic time, but can
be precisely proven to be. Other algorithms can do it
a bit faster.
</p>
<p>There are two basic classes of summation formulae: sum
of a power of integers, and sum of a geometric progression.
</p>
<p>Logarithms arise in problems where things are repeatedly
halved.
</p>

<h2>Chapter 3: Data Structures</h2>


<h2>Chapter 4: Sorting</h2>


<h2>Chapter 5: Divide and Conquer</h2>


<h2>Chapter 6: Hasing and Randomized Algorithms</h2>


<h2>Chapter 7: Graph Traversal</h2>


<h2>Chapter 8: Weighted Graph Algorithms</h2>


<h2>Chapter 9: Combinatorial Search</h2>


<h2>Chapter 10: Dynamic Programming</h2>


<h2>Chapter 11: NP-Completeness</h2>


<h2>Chapter 12: Dealing with Hard Problems</h2>


<h2>Chapter 13: How to Design Algorithms</h2>


<h2>Messages</h2>

<p>Anything tagged with <code>TADM</code> will show up here.
</p>
</div>
</body>
</html>
