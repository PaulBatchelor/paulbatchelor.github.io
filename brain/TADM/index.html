<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="/brain/css/style.css">

</head>
<body>
<div id="main">
<title>The Algorithm Design Manual</title>
<h1>The Algorithm Design Manual</h1>
<p>By Steven S. Skiena (Third Edition)
</p>
<p>UUID: <code>gipejljdq-eoqa-heio-wauq-idrilewkhhel</code></p>
<p>Recommended book in <a href="/brain/TYCS">TYCS</a>.
</p>
<p>it is divided into two parts, "techniques" and "resources".
The former is a general introduction to algorithm design,
while the latter is inteded for browsing and reference.
</p>

<h2>Chapter 1: Introduction To Algorithm Design</h2>

<p>An algorithm is a procedure to accomplish a specific task.
</p>
<p>An algorithm must solve a generalizable set of problems. An
algorithm will perform on an <code>instance</code> of a problem.
</p>
<p>Examples: Robot Tour Optimization (The Traveling Salesman
problem) and Selecting the Right Jobs.
</p>
<p>Reasonable looking algorithms can be easily incorrect.
Algorithm <code>correctness</code> is a property that must be
carefully demonstrated.
</p>
<p>Proofs are a tool used to distinguish correct algorithms
from incorrect ones. Formal proofs will not be emphasized
in this book.
</p>
<p>Correctness of an algorithm requires that the problem
be carefully and clearly stated. Problem specifications
have two parts: the set of allowed instances, and the
require properties of the algorithm's output.
</p>
<p>An important technique in algorithm design is to narrow
the scope of allowable instances until there is an
efficient solution.
</p>
<p>Avoid ill defined quetions, such as "best" routes (what
does best mean?). Also avoid compound goals.
</p>
<p>Three common forms of algorithmic notation: English,
pseudocode, real programming language.
</p>
<p>Pseudocode: best defined as a programming language that
never complains about syntax errors.
</p>
<p>The heart of an algorithm is an idea. If your idea
is not clearly revealed when you express an algorithm,
then you are using too low-level a notation to describe it.
</p>
<p>Counterexample: an instance for an algorithm which yields
an incorrect answer.
</p>
<p>Counterexample properties: verifiability and simplicity.
</p>
<p>Verifiability: demonstrate that it is a counterexample.
Calculate what the answer would have been, and display
a better answer to prove that the algorithm didn't find it.
</p>
<p>Simplicity: strips away all details, and clearly shows
why the algorithm fails.
</p>
<p>Strategies for finding counterexamples: think small,
think exhaustively, hunt for the weakness, go for a tie,
seek extremes.
</p>
<p>Searching for counterexamples is the best way to
disprove the correctness of a heuristic.
</p>
<p>Failure to find a counterexample to a given algorithm
does it mean "it is obvious" the algorithm is correct.
A proof or demonstration of correctness is needed, often
with the use of mathematical induction.
</p>
<p>Mathematicl induction is usually the right way to
verify the correctness of a recursive or incremental
insertion algorithm.
</p>
<p>Modeling: the art of formulating your application in
terms of precisely described, well understood problems.
</p>
<p>Common structures: permutations, subsets, trees, graphs,
points, polygons, strings.
</p>
<p>Permutations: likely the object in question for problems
that seek "tour, "ordering", or "sequence".
</p>
<p>Subset: likely needed when encountering problems that
seeks a "cluster", "collection", "committee", "group",
"packaging", or "selection".
</p>
<p>Trees: likely object in question when a problem seeks
"hierarchy", "dominance relationship",
"ancestor/descendant relationship", or "taxonomy".
</p>
<p>Graphs: "network", "circuit", "web", or "relationship".
</p>
<p>Points: "sites", "position", "data records", or "locations".
</p>
<p>Polygons: "shapes", "regions", "configurations",
"boundaries".
</p>
<p>Strings: "text", "characters", "patterns", or "labels".
</p>
<p>Modeling your application in terms of well defined
structures and algorithms is the most important single step
towards a solution.
</p>
<p>Thinking recursively: looking for big things that are made
up of smaller things of exactly the same type as the big
thing.
</p>
<p>Proof by Contradiction: assume the hypothesis is false,
develop some logical consequences of assumption, show
that one consequence is false.
</p>
<p>Estimation is principled guessing. Try to solve the problem
in different ways and see if the answers generally agree
in magnitude.
</p>

<h2>Chapter 2: Algorithm Analysis</h2>

<p>Studying algorithm efficiency without implementing them.
</p>
<p>Two tools for analysis: the RAM model of computation and
asymptotic analysis of computational complexity.
</p>
<p>RAM: Random Access Machine. Hypothetical computer machine
where operation time is measured in steps.
</p>
<p>In RAM, the quality of an algorithm is determined by how it
works over all possible instances.
</p>
<p>Best case, worst case, average case.
</p>
<p>Best case: minimum number of steps taken when instance
is size N.
</p>
<p>Worst case: maximum number of steps when instance is size
N.
</p>
<p>Average case: average number of steps when instance is
size N. AKA expected time.
</p>
<p>Worse Case tends to be the most useful in practice.
</p>
<p>Average Case is helpful when working with randomized
algorithms.
</p>
<p>Big-oh notation: simplifies analysis. More "big picture".
</p>
<p>Upper bound, lower bound, something inbetween bound (not
sure what to call this)?
</p>
<p>Big oh ignores difference between multiplicative constants.
</p>
<p>Dominance: faster growing functions dominate slower
growing ones.
</p>
<p>Functions are grouped by different classes, in descending
order: Factorial, Exponential, Cubic, Quadratic,
Superlinear, Linear, Logarithmic, Constant.
</p>
<p>Special considerations for thinking about adding/multipying
functions with Big Oh. (See book).
</p>
<p>Analysis for selection sort (See book for details). Proving
two ways that the Big Oh running time is quadratic (n^2).
</p>
<p>Analysis for insertion sort. Using a "round it up" approach
to find the upper bound worst-case Big Oh time. Useful
approximation for simple algorithm analysis.
</p>
<p>Finding a substring. Analyzes each looping component
of the algorithm, and using Big Oh rules to get
an expression interms of n and m.
</p>
<p>"proving the theta": proving that the time assessment is
correct.
</p>
<p>Matrix Multiplication. Smells like cubic time, but can
be precisely proven to be. Other algorithms can do it
a bit faster.
</p>
<p>There are two basic classes of summation formulae: sum
of a power of integers, and sum of a geometric progression.
</p>
<p>Logarithms arise in problems where things are repeatedly
halved.
</p>
<p>Binary search work in logarithmic time.
</p>
<p>Range of binary values  double when you add a bit.
</p>
<p>Logarithms used to be used to multiple very large
numbers by hand.
</p>
<p>End of the chapter here covers advanced analysis
techniques. Not used in the rest of the book.
</p>

<h2>Chapter 3: Data Structures</h2>


<h3>3.1 Contiguous Vs Linked Data structures</h3>

<p>Data structures can be classified as linked or contiguous.
</p>
<p>Contiguous: structures composed of a single slab of memory.
</p>
<p>Linked: structures composed of distinct chunks of memory
tied together using pointers.
</p>
<p>Array: fundamental contiguous data struture, consisting
of a fixed size data record where each element can be
addressed using an index.
</p>
<p>Contiguous array advantages: constant-time access, space
efficiency, memory locality.
</p>
<p>Dynamic Arrays: arrays that can efficiently expand. They
double in size every time more space is needed, and items
are copied over to the new chunk. Insertion takes constant
time worst case.
</p>
<p>Linked structures advantages: overflow never happens
unless memory is actually full, insert/delete are simpler
than static arrays, moving is easier with larger records.
</p>
<p>Dynamic memory allocation provides flexibility on how/where
limited storage resources are used.
</p>

<h3>3.2 Containers, Stacks and Queues</h3>

<p>Container: abstract data type that permits
storage/retrieval of items independent of context.
</p>
<p>Containers include stacks and queues.
</p>
<p>Stack: LIFO. Simple and efficient. Good to use when
retrieval doesn't matter. Push and pop.
</p>
<p>Queue: FIFO. Trickier to implement than stacks. Good
for applications where order is important. Minimizes
that maximum time spent waiting. Enqueue and Dequeue.
</p>
<p>Stacks/Queues can be implemented using either an array
or linked list.
</p>

<h3>3.3 Dictionaries</h3>

<p>Dictionary: data type that permits access to data items
by content.
</p>
<p>Dictionary operations: search, insert, delete.
</p>
<p>Other Dictionary operations: Max/Min: retrieve item
with largest or smallest key. Predecessor/Successor:
retrieve item that comes before or after given key in
sorted order.
</p>
<p>Data structure design must balance the operations it
supports. The fastest structure supporting two
operations may not be as fast as ones that support
them individually.
</p>
<p>Comparing times for sorted or unsorted array operations.
</p>
<p>Comparing times for sorted or unsorted and
doubly or singly linked list operations.
</p>

<h3>3.4 Binary Search Trees</h3>

<p>Rooted Binary Tree: recursively defined as being empty,
or consiting of a a node called the root, together
with two rooted binary trees called left and right.
</p>
<p>Binary Tree Search: runs in O(h) time, where <code>h</code> denotes
height of tree.
</p>
<p>Binary Tree Operations (with sample code): Search, minimum,
maximum, traversal, insertion, deletion.
</p>
<p>Binary Tree Insertion: exactly one place to insert an item
X into a tree T. Can be done recursively.
</p>
<p>Binary Tree Deletion: tricky because the two descendents
have to be linked into the tree somewhere else. Three
possibilities: leaf node, node with 1 child, node with
2 children. Deleting a node with 2 children is the most
difficult to handle correctly.
</p>
<p>Performance of binary tree on average has theta log(N),
assuming all possibilities are equally probable.
</p>
<p>Balanced Search Trees: Trees that adjust after each
insertion, guaranteeing that the height is always
O(log N).
</p>
<p>The key to binary search trees is exploiting them as
black boxes.
</p>

<h3>3.6 Priority Queues</h3>

<p>Priority Queue: an abstract data type that allows
elements to be added at arbitrary times and be sorted
properly.
</p>
<p>Three Primary Operations: Insert, Find-Minimum/Find-Maximum,
Delete-Minimum/Delete-Maximum.
</p>
</p>

<h3>3.6 War story: triangle stripping</h3>

<p>War Story: using priority queues with a dictionary
to find small triangle strips that cover a given mesh.
</p>

<h3>3.7 Hashing</h3>

<p>Hash tables: very practical way to maintain a dictionary.
</p>
<p>Hash function: a mathematical function that maps keys
to integers.
</p>
<p>Collisions: when two keys map to the same integer value.
</p>
<p>Chaining: represents hash table as array of linked lists.
Natural way to resolve collision, but memory is allocated
for pointers, rather than ways to make the table larger
(and lookup faster).
</p>
<p>Open Addressing: maintains hash table as simple array of
elements (not buckets like chaining). If an index position
is filled, find the next available one closest. This
has better performance, but deletion is difficult.
</p>
<p>Hashing is a good method for duplicate detection.
</p>
<p>Duplicate detection applications that can use hashing:
checking for a document in a large corpus, document
plagiaraization, proof that file hasn't been changed.
</p>
<p>Hashing tricks: fundamentally about many-to-one mappings,
where the many isn't too many.
</p>
<p>Canonicalization: reducing complicated objects to a
standard form. Hash tables can be used here.
</p>
<p>Intentional collisions in hash tables work well for
pattern matching problems.
</p>
<p>Fingerprinting: hashing for compaction.
</p>

<h3>3.8 Specialized Data Structures</h3>

<p>Basic Data Structures: represent an unstructured set
of items that facilitate retrieval operations.
</p>
<p>Less well known: data structures for representing
more specialized objects: points, strings, graphs, sets.
</p>
<p>Similar idea: each of these have operations which
need to be performed on them, efficiently.
</p>

<h3>3.9 War Story: Sring 'em up</h3>

<p>Finding substrings in a genome sequence (very large)!
</p>
<p>Binary search tree: good until it wasn't.
</p>
<p>Profiled. Faster dictionary structure. Use hash table:
good until it wasn't.
</p>
<p>Use a suffix tree: worked until it didn't. Ran out
of memory.
</p>
<p>Compressed suffix tree: things work now!
</p>
<p>A single operation was isolated (dictionary string search)
that was being performed many times, and a performance
was improved by studying the algorithm and finding
suitable data structures for it.
</p>

<h2>Chapter 4: Sorting</h2>

<p>Sorting is fundamental to many algorithms, the source
of many interesting ideas in the field of algorithm
design, and is the most thorougly studied problem in CS.
</p>

<h3>4.1 Application Sorting</h3>

<p>Clever sorting: O(N Log(N)).
</p>
<p>Naive: O(N^2).
</p>
<p>Sorting is often a basic building block in many algorithms.
Having things sorted tends to make things way easier.
</p>
<p>Some problems that take advantage of sorting: Searching,
Closest pair, Element uniqueness, Finding the mode,
Selection, Convex hulls.
</p>
<p>Stop and think: finding the intersection. 3 approaches
involving different combinations of sorting. Small-set
sorting is best. However, a hash table could be used
to optimally solve this problem.
</p>
<p>When to use hashes?
</p>
<p>Searching: hashes are a great answer here.
</p>
<p>Closest Pair: hashes (defined thus far) do not help.
</p>
<p>Element Uniqueness: hashing is faster than sorting for
this problem.
</p>
<p>Finding the mode: linear time with hash.
</p>
<p>Finding the medium: does not help.
</p>
<p>Convex Hull: not really.
</p>

<h3>4.2 Pragmatics of Sorting</h3>

<p>What order do we want items sorted?
</p>
<p>Increasing vs decreasing order.
</p>
<p>Key vs entire record.
</p>
<p>Handling equal keys.
</p>
<p>Non-numerical data.
</p>
<p>An application specific comparison function is used
to help solve these issues, which allows sorting algorithms
to be studied independent of their context.
</p>
<p>Qsort: C standard library sorting quicksort function that
takes in a comparison function.
</p>

<h3>4.3 Heapsort: fast sorting with data structures</h3>

<p>While you are better off using standard sorting functions,
understanding how they work will be helpful for
understanding how other algorithms work.
</p>
<p>Selection sort: impelementation takes O(n) to find
smallest item in unsorted list, O(1) to remove the item.
Total time takes O(n^2). Using priority queue for these
operations will take performance from O(n^2) to O(nlog(n))
time. Priority queue can be made with heap or balanced
binary tree.
</p>
<p>Heapsort: really just selection sort with the right data
structure.
</p>
<p>Heaps: data structure used for efficiently supporting
PQ operations insert and extract-minimum.
</p>
<p>heap-labeled tree: binary tree such that the key of each
node dominates the keys of its children.
</p>
<p>min-heap: domination happens when parent node has small key
than children.
</p>
<p>heap: allows one to convey trees without pointers. Data
is stored as array of keys.
</p>
<p>Acquiring parent/children in heap: left child of <code>k</code> sits
as <code>2k</code>, right <code>2k + 1</code>, parent of <code>k</code> is at <code>k/2</code>.
</p>
<p>Missing internal nodes of a sparse tree still take up
space.
</p>
<p>Elements in heap must be packed as far left as possible,
leaving empty elements only at the end.
</p>
<p>Heap is not a binary search tree, so one can't efficiently
search through the tree.
</p>
<p>To make a heap: insert at leftmost open position, then
"bubble up" (swapping with parent) to assert dominance.
Each insert has an O(log(n)) time, with a total insert
of O(n*log(n)) time.
</p>
<p>Extract minimum from heap: pop of the top of the heap,
then replace with rightmost node (nth position in array).
Bubble down to ensure that the heap property is
satisfied (the value dominates its children). This bubbling
down is also known as <code>heapify</code>.
</p>
<p>Faster heap construction is possible, using N/2 calls
to bubble down. It quickly converges to linear time
instead of O(n*log(n)).
</p>
<p>Stop and think: where in the heap?
</p>
<p>Insertion sort: O(n^2) time. simplest example of
incremental insertion.
</p>
<p>Incremental insertion: for n items, build on (n - 1)
items, then make changes to add last item.
</p>
<p>Faster incremental-insertion based sorting algorithms
utilize more efficient data structures, such as a balanced
search tree.
</p>

<h3>4.5 Mergesort: sorting by divide and conquer</h3>

<p>Mergesort: Break a list into two parts, sort, then merge
them together.
</p>
<p>Mergesort efficiency: dependent on how well one can combine
the two sorted halves into a single sorted list.
</p>
<p>Total runtime of mergesort: n / 2^k, where n is the number
of items (and a power of 2), and k is the level it is
being processed on.
</p>
<p>Linear work is done merging the elements at each level.
</p>
<p>Divide-and-conquer mergesort implementation closely
resembles pseudo-code.
</p>
<p>Merging is more challening: where to put the merged
array? Solution is to copy subarray to avoid overwrite,
then merge back into array.
</p>

<h3>4.6 Quicksort: Sorting by Randomization</h3>

<p>Quicksort: take an item from set (pivot), and
split (parition) remaining items into two piles: those
greater and those lesser. Pivot item ends up in exactly
the position it is supposed to be in. Go into each pile,
pivot and repeat.
</p>
<p>Quicksort is a recursive sorting algorithm.
</p>
<p>Quicksort runs in O(n*h) time, where <code>h</code> is the height
of the recursion tree.
</p>
<p>The height of the quicksort recursion tree can vary,
depending on where the pivot ends up.
</p>
<p>Best case is it
picks the median every time, where each level is half
the size of the previous level (h = log(n)).
</p>
<p>Worst case is picking the most uneven pivot point in
subarray (biggests or smallest value). This gives a
worst case time of n^2. This worst case time is
worst than heapsort or mergesort.
</p>
<p>Expected performance of quicksort is O(n * log(n)) time.
</p>
<p>On average, random quicksort trees perform well. The odds
of running into a worst case of quadratic time are very
small.
</p>
<p>By scrambling the order of a list before applying
quicksort (adds an addition O(n) time), it guarantees
that any input will have a high probability of running
in O(n*log(n)) time.
</p>
<p>In general, the use of randomization can help to improve
algorithms with bad worst-case, but good worst-case
complexity.
</p>
<p>Randomization techniques: random sampling, randomized
hashing, randomized search.
</p>
<p>Stop and think: nuts and bolts.
</p>

<h3>4.7 Distribution Sort: Sorting via Bucketting</h3>

<p>Example: organizing names in a phonebook. Break names
into sections by letter, with sections arrangement in
alphabetical order. In each section (bucket), do the same
with the second letter. Repeat until each bucket has
one letter.
</p>
<p>Bucketting works when data is expected to be roughly
uniform.
</p>
<p>Sorting algorithms based on comparison cannot work in
linear time.
</p>
<p>Sorting can be used to illusrate many design paradigms,
such as data structure techniques, divide and conquer,
randomization, and incremental construction.
</p>

<h2>Chapter 5: Divide and Conquer</h2>

<p>Divide and conquer: divide a problem up into two smaller
subproblems, recursively solve, then meld them together.
</p>
<p>When merging is more efficient than the problem solving,
it is an efficient algorithm.
</p>

<h3>5.1 Binary search and related algorithms</h3>

<p>Binary Search: fast algorithm for searching in a sorted
array of S. Key can be located in O(log(n)) time.
</p>
<p>One sided binary search: search algorithm on an
array that divides it into segments of increasing size.
Is able to find the transition point p in 2*log(p)
comparisions. Useful when trying to find item that is
close to current position.
</p>
<p>Bisectional Method: method used to find the square root
of a number. Can also be generalized means of finding
roots to an equation. Finds a midpoint, tests the function,
the updates the midpoint again and repeats until an
answer is found.
</p>

<h3>5.2 War story: finding the bug in the bug</h3>

<p>Using binary search to figure out what was causing a
sequenced (biological) virus to die rather than be
weakened (for use in synthetic vaccine).
</p>
<p>Binary search can be done in parallel, provided that
queries are arbitrary subsets rather than connected
halves.
</p>

<h3>5.3 Recurrence Relations</h3>

<p>Recurrence Relation: an equation in which a function is
defined in terms of itself.
</p>
<p>Recurrence relations are related to recursion in
programming, and enable one to formally analyze recursive
functions.
</p>
<p>Divide-and-conquer can be represented in the following
recurrence relation T(n) = a*T(n/b) + f(n). Where
a task <code>T</code> is broken up in <code>a</code> problems, each being of
size <code>n/b</code>, and then merged in time <code>f(n)</code>.
</p>
<p>Solving a recurrence: finding a nice closed form
describing or bounding the result.
</p>

<h3>5.4 solving divide-and-conquer recurrances</h3>

<p>Divide-and-conquer form usually falls into three distinct
cases, described mathematically (presented in book). Known
as the <code>master theorem</code>.
</p>
<p>Three cases of master thereom correspond to three different
costs, each of which would be dominant as a function of
a, b, and f(n): too many lives (case 1), equal work per
level (case 2), too expensive a root (case 3).
</p>
<p>If you accept the master theorem, you can easily any
divide-and-conquer algorithm given only the recurrence
associated with it.
</p>

<h3>5.5 Fast multiplication</h3>

<p>Faster multiplication algorithms can be obtained by
defining a recurrence that uses fewer multiplications
but more additions.
</p>

<h3>5.6 Largest Subrange and Closest Pair</h3>

<p>Largest subrange: given a set of integers, which range
yields the largest sum? Using divide and conquer, one
can do it in O(n * log(n)) time.
</p>
<p>Broad strokes approach: find the best on each side, then
check what is straddling in the middle.
</p>
<p>If the largest subregion is somewhere in the middle: it's
the union of two subregions in the left and right, left
subregion ends on <code>m</code>, right subregion starts on <code>m + 1</code>.
</p>
<p>Dividing into two halves and sweeping does linear work
with a recurrence of $T(n)=2*T(n/2) + \theta(n)$. Case
2 of the master theorem yields $T(n) = \theta(n \log(n)).
</p>
<p>The problem of finding the smallest distance between two
points can be reworked from a linear sweep
with n*log(n) time into into a divide-and-conquer with
linear time, and proven by defining the recurrence and
applying the master theorem. Using a similar process,
it can be shown that the case for 2d points can be found
in n*log(n) time.
</p>

<h3>5.7 Parallel Algorithms</h3>

<p>Divide and conquer is the most suitable algorithm for
parallel computation.
</p>
<p>Parition a problem of size <code>n</code> into <code>p</code> equal parts,
thus yielding a time of <code>T(n/p)</code>.
</p>
<p>Most parallel tasks aim to exploit <code>data parallelism</code>:
multiple tasks each process separate data.
</p>
<p>Most parallel algorithms are simple yet effective.
</p>
<p>Pitfalls of parallelism: there is a small uper bound
on the potential win, speedup mean snothing, parallel
algorithms are tough to debug.
</p>

<h3>5.8 War Story: Going Nowhere Fast</h3>

<p>Parallelizing the Waring's conjector algorithm from
the previous chapter for use on a supercomputer. Lots
of problems ensue.
</p>
<p>Moral of the story: there are subtleties to consider
when adapting an algorithm to be parallel. Taking the
time to do proper load balancing is important.
</p>

<h3>5.8 Convolution</h3>

<p>There is a divide-and-conquer strategy for sorting that
can prevent it from being a quadratic time operation.
</p>
<p>Important operations of convolution: integer
multiplicatioin, cross-correlation, moving average filters,
string matching.
</p>

<h2>Chapter 6: Hashing and Randomized Algorithms</h2>

<p>Why Randomize? Yields new classes of efficient algorithms
that relax the requirement of always correct or always
efficient.
</p>
<p>Two main types of randomized algorithms: Las Vegas
algorithms and Monte Carlo algorithms.
</p>
<p>Las Vegas Algorithm: guarantee correctness, but not
always efficient. Ex: quicksort.
</p>
<p>Monte Carlo Algorithm: Probably efficient, but not always
correct. Ex: random sampling methods.
</p>
<p>While often easy to implement, randomized algorithms can
be difficult to analyze rigorously.
</p>
<p>Randomized algorithms are formally analyzed using
probability theory.
</p>

<h3>6.1 Probability Review</h3>

<p>Probability Theory: formal framework for reasoning about
the likelihood of events.
</p>
<p>Experiment: a procedure that yields one of a set of
possible outcomes.
</p>
<p>Sample space: set of possible outcomes of an experiment.
</p>
<p>Event: a specified subset of the outcomes of an experiment.
</p>
<p>Set Difference: A - B. Operation that produces
outcomes of event A that are not outcomes of event B.
</p>
<p>Intersection: Outcomes between both event A and event B.
</p>
<p>Union: outcomes that appear in either A or B.
</p>
<p>Events A and B are said to be independent if there is
no special structure of shared outcomes between events.
</p>
<p>Indpendent events are ideal in probability theory because
they simpilify calculations.
</p>
<p>Conditional Probability: the probability of A, given B.
</p>
<p>Conditional probability is only interesting when the events
are depedent on eachother.
</p>
<p>Baye's Theorem: reverses the direction of dependencies.
</p>
<p>Probability Density Function: a way of representing random
variabnles. Known as PDF.
</p>
<p>Two main types of summary statistics: central tendency
measures, and variation or variability measures.
</p>
<p>Central tendency measures: capture the center around
which datum is distributed.
</p>
<p>Variability measures: spread or how far datum lies from
the center.
</p>
<p>Primary centrality measure is the mean.
</p>
<p>Most common measure of variability is the standard
deviation.
</p>
<p>Variance is the square of the standard deviation.
</p>
<p>Stop and Think: Random Walks on a Path.
</p>

<h3>6.2 Understanding Balls and Bins</h3>

<p>Balls and bins are a classic probability model: X balls
to toss in Y random bins. Interest is in the distribution
of the balls.
</p>
<p>Precise analysis of random process requires formal
probability theory, algebraic skills, and careful
asymptotics.
</p>

<h3>6.3 Why is Hashing a Randomized Algorithm</h3>

<p>Randomized algorithms make the worst-case scenarios go
away.
</p>
<p>Hashing functions are deterministic. In theory one could
exploit a particular hash function with a edge-case.
</p>
<p>Constructing a hash function at random prevents worst-case
intentional inputs from happening.
</p>

<h3>6.4 Bloom Filters</h3>

<p>Bloom Filter: Bit-vector hash table. When a bit is set,
it means there's something in the bucket. Takes an
item, hashes it in K different ways, then puts each result
into a bucket.
</p>
<p>An already false positive collision in the bloom filter
would need to set the bits for all the hash functions used.
The probability of this happening is calculated in the
book.
</p>

<h3>6.5 The Birthday Paradox and Perfect Hashing</h3>

<p>Perfect Hashing: a guaranteed
worst-case constant time search that works for static
dictionaries.
</p>
<p>Calculate: how large a hash table do you need before you
get zero collisions?
</p>
<p>Relates to the Birthday Paradox: how many people do you
need in a room before it is likely that at least two
of them share the same birthday?
</p>
<p>Perfect hashing utilizes a 2-level hash table.
</p>
<p>Perfect hashing is ideal for when you are making a large
number of queries in a static dictionary.
</p>
<p>Minimum perfect hasing: guarantees constant time access
with zero empty hash table slots, resulting in an n-element
second hash table for n keys.
</p>

<h3>6.6 Minwise hashing</h3>

<p>Jaccard Similarity: means of measuing similarity between
two items. The similarity measure outputs 0-1, which
can be thought of as a probability that the documents
are similar. See equation in book.
</p>
<p>How to compare similarity of two documents without looking
at every word? Minwise hashing.
</p>
<p>Minwise hashing: compute the hash value of one document,
get word with the smallest hash value. Then do the same
with the other document using the same function.
</p>
<p>The probabability that the minhash word appears in both
documents depends on the words in common, as well as
the total size of the documents.
</p>
<p>Minhash value: building indexes for similarity search and
clustering over a collection of documents.
</p>

<h3>6.7 Efficient String Matching</h3>

<p>String primary data structure: array of characters, which
allows constant-time access to parts of the string
via indexes.
</p>
<p>Substring search: fundamental operation on text strings.
Does an input text string contain a pattern inside of it,
and if so, where?
</p>
<p>Rabin-Karp Algorithm: general-purpose substring matching
algorithm based on hashing with linear expected time.
</p>
<p>When a test substring is moved by one character, the hash
of the current substring and the previous is different
by one character, and thus can be computed from the
previous hash using two multiplications, one addition,
and one subtraction.
</p>

<h3>6.8 Primality Testing</h3>

<p>Primality: is the number a prime number?
</p>
<p>Randomized algorithms can be used for primality testing,
and turn out to be faster than the factoring approach.
</p>
<p>Fermat's little theorem: $a^{n-1} = 1(\mod n)$ for all
a not divisible by n.
</p>
<p>The mod of this big power is always 1 if n is prime. The
odds of it being 1 by chance is very small.
</p>
<p>Pick 100 random integers between 1 and (n - 1). If all
random numbers come out to be 1, it is probably prime,
and the chances of it <b>not</b> being prime are very small.
This is a monte-carlo type of randomized algorithm.
</p>
<p>Carmichael Numbers: Numbers that aren't prime, but fit
the Fermat congruence for all "a". These will act as
false positives in the randomized primality algorithm
described above.
</p>

<h3>6.9 Giving Knuth the Middle Initial</h3>

<p>Author finds a problem in TAOCP suggestive of Fermats
little theorem, and writes a mathematica program that
refutes the conjecture. Contacts Knuth, and Knuth tells
him that next edition will use his name. Asks for middle
initial.
</p>

<h3>6.10 Where do Random Numbers Come from?</h3>

<p>A Linear Congruent Generator (LCG) is essentially a hash
function that is typically used to produce random
numbers for a computer.
</p>

<h2>Chapter 7: Graph Traversal</h2>

<p>A graph consists of a set of vertices V together with a
set E of vertex pairs or edges.
</p>

<h4>7.1 Flavors of Graphs</h4>

<p>Flavors: Undirected vs Directed. Weighted vs. Unweighted.
Simple vs. Non-simple. Sparse vs. Dense. Cyclic vs.
Acyclic. Embedded vs. Topological. Implicit vs. Explicit.
Labeled vs. Unlabeled.
</p>
<p>Undirected/directed: directed if edges
(x,y) and (y, x) exist. AKA does it flow both ways or not?
</p>
<p>Weighted/unweighted: Do vertices in a graph have some
concept of "weight" associated with them? Finding the
shortest path in a weighted graph requires more
sophisticated algorithms.
</p>
<p>Simple/non-simple: graphs with complicated edges, such
as a self-loop (x,x) and a multi-edge (edge occurs more
than once).
</p>
<p>Sparse vs Dense: How many defined pairs are there? A
complete graph contains all possible pairs.
</p>
<p>Cyclic vs. Acyclic: cylic is a closed path of 3 or more
vertices that do not repeat except for tghe start and
end point. Trees are undirected and acyclic graphs.
Directed acyclic graphs (DAGs) are commonly found in
scheduling problems with dependency chains.
</p>
<p>Embedded/Topological: Embedded means that vertices or
edges are assigned geometric positions. Edge and Vertex
descriptions are purely topological.
</p>
<p>Implicit/explicit: Some graphs do not need to be fully
constructed, and can be built on-demand as needed.
</p>
<p>Labeled/unlabeled: vertexes are given a unique ID or name
to distinguish it from other vertices.
</p>
<p>Degree of Vertex: number of edges adjacent to it.
</p>
<p>Graphs can be used to model a wide variety of
structures an relationships. Graph-theoretic terminology
gives us a language to talk about them.
</p>

<h3>7.2 Data Strutures for Graphs</h3>

<p>Main choices for graph data structure are: adjacency
matrices and adjacency lists.
</p>
<p>Adjacency Matrix: Use an NxN matrix to represent every
possible pair. Can quickly check if two vertices are
connected as a pair, but can take up a lot of space.
</p>
<p>Adjacency List: use linked list to store neighbors of
each vertex. Typically more compact, and take more time
verify whether a given edge is in a graph. Usually
the correct choice for a Graph data structure.
</p>

<h3>7.3 War Story: I was a Victim of Moore's Law</h3>

<p>Development of Combinatorica (author's library for
mathematica) and changing the way graphs are represented
over the years due to hardware improvements.
</p>

<h3>7.4 War Story: Getting The Graph</h3>

<p>Problem related to extracting triangular strips for
fast rendering of triangular surfaces, modelled
as a graph problem. It turns out that there was a
bottleneck was in the construction of the graph which
had to be addressed (it was working in quadratic time).
After fixing the graph construction, the graph could
be built in seconds instead of minutes.
</p>

<h3>7.5 Traversing a Graph</h3>

<p>Traverse a Graph: visit every edge and vertex in a
systematic way.
</p>
<p>Efficient Traversal: don't visit the same places
repeatedly.
</p>
<p>Corrrect Traversal: Ensure that every vertice/edge gets
visited.
</p>
<p>Mark as you go. Each vertice has three possible states:
undiscovered, then discovered, and then processed.
</p>
<p>Undirected edges will be considered twice (both ends).
Directed edges will be considered once.
</p>

<h3>7.6 Breadth-First Search</h3>

<p>BFS on undirected graphs assigns direction from discoverer
to the discovered. Each node has exactly
one parent (except for root). This creates a tree.
</p>
<p>For nontree edges, can point only to vertices on the same
level as the parent vertex, or to vertices directly below
the parent.
</p>
<p>BFS Implementation: two boolean arrays to keep track
of vertices in graph: discovered and processed.
</p>
<p>A discovered vertex is placed on a FIFO queue. Oldest
vertices are expanded first, which are closest to
the root.
</p>
<p>Parent array: useful for finding paths inside the graph.
Parent that first discovered vertex <code>i</code> is <code>parent[i]</code>.
</p>
<p>The tree that BFS produces paths that are the shortest
route from the root to a particular value. Path can be
reconstructed for any given value X by following X back
to the root.
</p>

<h3>7.7 Applications of Breadth-First Search</h3>

<p>Properly implemented traversals are usually around
linear time, or O(n + m) with n vertexs and m edges.
</p>
<p>Graph is connected is there is a path between any two
vertices.
</p>
<p>Connected component: maximal set of vertices such that
there is a path between every pair of vertices. These
components are self-contained in a graph, and do not
connect to other components.
</p>
<p>Many (seemingly) complicated problems reduce down to
finding connected components in a graph structure.
</p>
<p>BFS can be utilized to find connected components.
</p>
<p>vertex-color problem: color each vertice of a graph such
that no edge links any two vertices of the same color,
using the least amount of colors.
</p>
<p>bipartite: graph that can be colored using only two colors.
</p>
<p>BFS mod to make bipartite graph: discovering a new
vertex means making it a color the opposite of its parent.
Check non-tree edge links to make sure colors don't match.
A successful traversal means the graph is colored
to be bipartite.
</p>

<h3>7.8 Depth-First Search</h3>

<p>Difference between DFS and BFS is in the order which they
explore vertices. Order dependent on the container data
structure to store the discovered but not processed
vertices: queue vs stack.
</p>
<p>In a Queue (FIFO), the oldest unexplored vertices are
explored first. This defines a BFS.
</p>
<p>A Stack (LIFO) data structure explores vertices along
a path, then backing up when it encounters already
discovered vertices. This defines a DFS.
</p>
<p>DFS can can be defined recursively, removing the need
for an explicit stack.
</p>
<p>Entry/exit times in DFS trees have interesting properties:
who is an ancestor? and how many descendants?
</p>
<p>DFS partitions edges of undirected graph into two classes:
tree edges and back edges.
</p>
<p>tree edges: discover new vertices.
</p>
<p>back edges: edges whose other endpoint is an ancestor
of the vertex being expanded, so they point back into
the tree.
</p>

<h3>7.9 Applications of Depth-First Search</h3>

<p>DFS isn't intimidating, but it is subtle. Correctness
requires getting details right.
</p>
<p>Correctness: dependent on when vertices and edges are
processed.
</p>
<p>Process a vertex before traversal, or after. Sometimes
things happen during both those times.
</p>
<p>Special attention is given to edges in undirected graphs:
(x,y) and (y, x) exist. Processed vertice is simple
case, discovered requires a bit more thought.
</p>
<p>Y is first traversal unless it is the immediate ancestor
of X (aka a tree edge).
</p>
<p>Cycle detection is done by checking for back edges. In
order for this to work, each edge in a traversal algorithm
must be processed exactly once.
</p>
<p>Connectivity of a graph: smallest number of vertics whose
deletion will disconnect the graph.
</p>
<p>Articulation Vertex: single vertex whose deletion
disconnects a connected component of the graph.
</p>
<p>A graph with an articulation vertex is said to have a
connectivity of 1.
</p>
<p>Test connectivity with brute force: remove a vertice,
then do DFS or BFS and see if graph iss still connected.
Total time is O(n(n + m)).
</p>
<p>Three reasons a vertex might be an articulation vertex:
root cut-nodes, bridge cut-nodes, parent cut-nodes.
</p>
<p>Root cut-nodes: root has two or more children.
</p>
<p>Bridge cut-nodes: earliest reachable vertex from v
is v.
</p>
<p>Parent cut-nodes: earliest reachable vertex from v is
the parent of v.
</p>

<h3>7.10 Depth-First Search on Directed Graphs</h3>

<p>DFS on Directed Graphs can encounter more types of
edges: forward, backward, cross. Undirected only encounters
back.
</p>
<p>Edge Classification can be determined by looking at
state, discovery time, and parent of each vertex.
</p>
<p>Topological Sort: operation performed on directed acyclic
graphs (DAGs). Orders vertices such that all directed
edges go from left to right.
</p>
<p>There can be more than one kind of topological sort.
</p>
<p>Topological sort allows vertices to be processed before
successors.
</p>
<p>Topological sort can be efficiently performed using DFS.
</p>
<p>Strongly connected: a directed graph that has a directed
path between any two vertices.
</p>
<p>Transpose Graph: A graph, but with all the edges reversed.
</p>
<p>Transposed Graph is used to establish if a Graph is
strongly connected. (Can be done in linear time!).
</p>

<h2>Chapter 8: Weighted Graph Algorithms</h2>

<p>Weighted Graphs: Graphs where edges have different values
or weights associated with them.
</p>

<h3>8.1 Minimum Spanning Trees</h3>

<p>Spanning Tree: a subset of edges from a graph that connect
to all vertices in a graph.
</p>
<p>Minimum Spanning Tree: Spanning tree with the smallest
possible combined weight.
</p>
<p>There can be more than one MST of a given graph.
</p>
<p>Algorithms below use greedy heuristics to find MST.
</p>
<p>Prim's Minimum Spanning Tree Algorithm: starts from
one vertex and grows the tree one edge at a time until
all vertices are included.
</p>
<p>Kruskal's Algorithm: another algorithm for
MST finding that uses greey heuristics. Builds up
connected components of verticices, culminating in MST.
Repeatedly considers lightest edge. If two endpoints end
up in same component, discard. Otherwise, merge components
into one.
</p>
<p>Performance of Kruskals Algorithm: <code>O(m*lg(m))</code> time for
sorting <code>m</code> edges, <code>O(m*n)</code> algorithm total.
</p>
<p>Union-find can be used to speed up component test, making
Kruskal's algorithm run in <code>O(m*lg(m))</code> time. Faster
than Prim's algorithm for sparse graphs.
</p>
<p>Set Partition: Itemizes the elements of some universal
set (ex: 1 to n) into a collection of disjoint subsets,
where each element is in exactly one subset.
</p>
<p>Connected components in a graph can be represented as
a set partition.
</p>
<p>To perform well, Kruskals algorithm needs these
operations to be efficient: Same Component Test, and
a Merge of two components.
</p>
<p>Union Find: data structure that represents each subset
as a "backwards" tree, with pointers from a node
to its parent.
</p>
<p>Previous operations (same component test and merge) can
be simplified into operations find and union,
respecitvely.
</p>
<p>Maximum Spanning Tree: Opposite of minimum spanning tree.
Can be obtained by negating weights and running Kruskal's
or Prim's algorithm.
</p>
<p>Minimum Product Spanning Tree: Spanning tree that minimizes
the product of edge weights (assuming all positive).
Replacing weights with their logarithms and then finding
the MST will accomplish this. Reasoning: logarithm identity
is the log of two multiplied values is equal to the sum
of the log values processed individually.
</p>
<p>Minimum Bottleneck Spanning Tree: Spanning tree that
minimizes the maximum edge weight over all possible trees.
</p>

<h3>8.2 War Story: Nothing but Nets</h3>

<p>Finding the Traveling Salesman problem in moving robot
arms and minimizing distance. Applying the problem to
graph algorithms to find reasonable solutions.
</p>
<p>Take home lesson: Most applications of graphs can be reduced
to standard graph properties where well known graphs can
be used, such as MSTs and shortest paths.
</p>

<h3>8.3 Shortest Paths</h3>

<p>Path: sequences of edges connecting two vertices.
</p>
<p>Shortest Path: Path that minimizes the sum of edge weights.
</p>
<p>Shorteset path between two vertices in
<b>unweighted</b> graph can be found using
a BFS starting from one of the points. Search tree
records the minimum-link path, which is therefore the
shortest path.
</p>
<p>Dijkstra's Algorithm: Preferred method for finding the
shortest path in weighted graph. Finds the shortest path
from starting point to all other destinations, including
the target destination.
</p>
<p>Dijkstra's algorithm is very similar to Prim's algorithm,
with the difference being the way the measure the
desirability of each vertex.
</p>
<p>Dijsktra's algorithm can be realized by only slightly
modifying the implementation for Prim's algorithm done
previously.
</p>
<p>Performance of Dijstra's algorithm: <code>O(n^2)</code>, just like
Prim's algorithm.
</p>
<p>Try to design graphs, not algorithms.
</p>
<p>Graph Diameter: the largest shortest-path over all pairs
of vertices.
</p>
<p>Floyd's algorithm: can be used to find the all-pairs
shortest path, using a graph stored in an adjacency
matrix.
</p>
<p>The performance of Floyd's algorithm is O(n^3), but
the implementation has very tight loops so it
performs well.
</p>
<p>Another application of Floyd's algorithm is that of
computing transitive closure.
</p>

<h3>8.4 War Story: Dialing For Documents</h3>

<p>Using weighted graphs to type in
words for phone dialing services.
</p>
<p>Lesson: the constraints for many pattern recogonition
problems can be naturally formulated as shortest-path
problems in graphs.
</p>

<h3>8.5 Network Flows and Bipartite Matching</h3>

<p>Network Flow Problem: asks for the maximum amount of flow
that can be sent from one vertice to another, while
respecting max capacities of each pipe.
</p>
<p>Matching: Subset of edges such that no two vertices share
a vertex. Pairs off vertices such that a vertice is in
at most one pair.
</p>
<p>Bipartite or  two-colorable: vertices can be devided into
two sets, L and R, such that all edges in G have
one vertex L and one vertex in R.
</p>
<p>Augmenting Paths: finding a path of positive
capacity from s to t, adding it to the flow.
</p>
<p>Flow through network is optimal if and only if there
is no augmenting path.
</p>
<p>Repeatedly applying path augmentation increases flow until
no such path remains, which produces the global maximum.
</p>
<p>Residual Flow Graph: defined as a weighted Graph G, where
weights represent capacity, and f is an array of flows
through G.
</p>
<p>Max flow from s to t always equal the weight of the minimum
s-t cut.
</p>
<p>Edmonds and Karp proved that always selecting the shortest
unweighted augmented path guarantees $O(n^3)$. Their
algorthm is what is implemented in this textbook.
</p>

<h3>Randomized Min-Cut</h3>

<p>Minimum-Cut Problem: aims to partition the vertices of
a graph into two sets, such that the smallest possible
number of edges apan across these two sets.
</p>
<p>Minimum-Cut in Network Reliability: what is the smallest
failure set whose deletion will disconnect the graph?
</p>
<p>If it takes K edge deletions to disconnect a graph, each
vertex must be connected to at least K other vertices.
This also impliies that for n vertices,
there are kn/2 edges.
</p>
<p>Contraction Operation: collapses vertices X and Y into
a single vertice XY. Any edge (X,Z) or (Y,Z) becomes
(XY, Z), and if both exist, two copies are made. An edge
(X, Y) gets replaced by a self loop (XY, XY).
</p>
<p>Contraction and Minimum Cut Size: size is unchanged unless
one we contract one of the K edges of the optimal cut,
in which case the min-cut size might grow because the
best partition is no longer available.
</p>
<p>Randomized algorithm: pick an edge and contract it, and
repeat this $n - 2$ times until there is a 2-vertex
graph with multiple parallel edges between them. Repeat
this procedure many times and report the smallest
cut as the minimum cut.
</p>
<p>The probability of this algorithm succeeding can be
calculated. The equation is presented in the book.
</p>
<p>It turns out that running the process $n^2log(n)$ times
returns a high probability of finding the minimum cut
at least once.
</p>
<p>The key to success in a randomized algorithm is setting
up a problem where the probabilities can be bounded and
formally analyzed.
</p>

<h3>8.7 Design Graphs, not algorithms</h3>

<p>Previous examples where designing graphs is better than
designing algorithms: maximum spanning tree as a minimum
spanning tree of a negative graph, and bipartite matching
using a special network flow graph.
</p>
<p>Lots of "Stop and Think" examples here. Will not list them
here, but are worth looking at. These show different
problems that can be solved by constructing a graph.
</p>
<p>Designing novel graph algorithms is very hard, so don't do
it. Instead, try to design graphs that enable you to use
classical algorithms to model your problem.
</p>

<h2>Chapter 9: Combinatorial Search</h2>

<p>Solving problems with exhaustive search techniques can
take a lot of computation, but can sometimes be worth it.
</p>
<p>Larger problems solved this way requires careful pruning of
the search space: only look for what matters.
</p>
<p>Backtracking as a technique for listing all possible solutions
to a combinatorial algorithm problem.
</p>

<h3>9.1 Backtracking</h3>

<p>Backtracking: systematic way to run through all the possible
configurations of a search space.
</p>
<p>Problems of this domain require that each possible combination
must be generated exactly once.
</p>
<p>Combinatorial search solution will be modelled as a vector,
where each element is selected from a finite ordered set.
</p>
<p>Each step of the backtracking algorithm, try to extend a
given partial solution, then test if it is a complete
solution.
</p>
<p>Backtracking constructs a tree of partial solutions, where
each node represents a partial solution.
</p>
<p>Backtracking ensures correctness by enumerating all
possibilities.
</p>
<p>Application-specific parts of implementation:
<code>is_a_solution</code>, <code>construct_candidates</code>, <code>process_solution</code>,
<code>make_move</code>.
</p>

<h3>9.2 Examples of Backtracking</h3>

<p>Constructing all subsets: set up a boolean array of N cells,
where each value signifies if it is in the subset. $S_k$ is
(true, false), and $a$ is a solution whenever $k = n$.
</p>
<p>Constructing all permutations.
</p>
<p>Constructing all paths in a Graph.
</p>

<h3>9.3 Search Pruning</h3>

<p>Pruning: The technique of abandoning a search direction
the instant it can be established that a given partial
solution cannot be extended into a full solution.
</p>
<p>Exploiting symmetry: pruning away partial solutions
equivalent to those previously considered.
</p>

<h3>9.4 Sudoku</h3>

<p>Backtracking lends itself nicely to solving sudoku problems.
</p>
<p>Heuristics to select the next square: arbitrary square
selection, or most constrained square selection (better).
</p>
<p>If the most constrained square has two possibilities left,
there is a 50% chance of getting it right on the first
guess.
</p>
<p><code>possible_values</code> approaches: local count or look ahead.
</p>
<p>Looking ahead to eliminate dead positions
as soon as possibnle is the best way to prune a search.
</p>
<p>Smart square selection had a similar impact, even
though it nominally just re-arranges the order in which
we do work.
</p>
<p>Even simple pruning strategies can suffice to ruduce running
times from impossible to instantaneous.
</p>

<h3>9.5 War Story: Covering Chessboards</h3>

<p>(I don't really know how to play chess, so I skimmed this.
basically using careful pruning to solve a problem related
to chess that was unsovled for 100 years)
</p>

<h3>9.6 Best First Search</h3>

<p>Explore your best options before less promising ones to speed
up search.
</p>
<p>Existential Search Problems: look for a single solution
satisfying a set of constraints.
</p>
<p>Optimization Problems: Seek the solution with lowest or
highest value of some objective function.
</p>
<p>Best-first search or Branch and Bound: assigns a cost
to every partial solution generated, and places them in
a priority queue so the most promising partial solutions
can be easily identified and expanded.
</p>

<h3>9.7 The A* Heuristic</h3>

<p>A* heuristic: (pronounced "A-star") is an elaboration on
branch-and-search. Use a lower bnound on the cost of all
possible partial solution extensions that is stronger
than just the cost of the current partial tour.
</p>
<p>The promise of a given partial solution is not just its
cost, but also includes the potential cost of the
remainder of the solution.
</p>

<h2>Chapter 10: Dynamic Programming</h2>

<p>The most challenging algorithmic problems involve
optimization: seeking to find a solution that maximizes
or minimizes an objective function.
</p>
<p>Algorithms for optimziation problems require proof that
they <b>always</b> return the best solution.
</p>
<p>Dynamic programming provides a way to design custom
algorithms that systematically search all possibilities
(guarantees correctness this way), while storing
intermediate results to avoid recomputing (efficient).
</p>
<p>Dynamic programming is a technique for efficiently
implementing a recursive algorithm by storing partial
results.
</p>
<p>Requires seeing that a naive recursive problem computes
the same problem over and over again.
</p>
<p>Start with a recursive algorithm or definition, then speed
it up with a results matrix.
</p>
<p>Suitable for optimization problems on combinatorial objects
with left-to-right order among components.
</p>
<p>Left-to-right order: strings, rooted trees, polygons, integer
sequences.
</p>

<h3>10.1 Caching vs. Computation</h3>

<p>Dynamic Programming is a tradeoff of space for time.
</p>
<p>Fibonacci Algorithm using Recursion: takes exponential
time to run.
</p>
<p>Fibonacci algorithm can perform much better by caching
results in a table, a technique known as memoization.
</p>
<p>Running time of cached fibbonacci algorithm runs in
linear time.
</p>
<p>Storing partial results doesn't work for these kinds
of recursive problems: quicksort, backtracking, depth-first
search. The recursive calls all have distinct parameter
values. Used once, and never again.
</p>
<p>Explicit caching of results of recursive calls provides
<b>most</b> of the benefits of dynamic programming, usually
including the same running time as the more elegant full
solution.
</p>
<p>Fibonacci dynamic progamming: calculating in linear time
can be done by explicitly specifying the order of evaluation
of the recurrence relation. With proper analysis, storage
time can be reduced to constant space with no asymptotic
degredation in running time.
</p>
<p>Binomial Coefficients: most important class of counting
numbers, where $(n \over k)$  counts the number of ways
to choose k things out of n possibilities.
</p>
<p>Binomial Coefficients can be computed using factorials,
but this can cause arithmetic overflow. A more stable
way to compute it is to use the implicit recurrence relation
in the construction of Pascal's Triangle (book has diagram).
</p>
<p>In Pascal's Triangle, each number is the sum of two numbers
directly above it.
</p>

<h3>10.2 Approximate String Matching</h3>

<p>Cost function tells how far apart two strings are. Distance
measures the number of changes required to convert one
string to another.
</p>
<p>Three natural types of changes: substitution, insertion,
deletion.
</p>
<p>Edit Distance: assign each operation an equal cost of 1.
</p>
<p>Recursive Algorithm Observation: last character in the
string must either be matched, substituted, inserted,
or deleted.
</p>
<p>Edit Distance by Recursion: very very slow. takes
exponential time.
</p>
<p>Edit Distance functin can be improved via a table-based
dynamic programming implementation.
</p>
<p>String comparison function returns the cost of the optimal
alignment, but not the alignment itself. The sequence
of editing operations still needs to be known.
</p>
<p>Decisions are recorded in the matrix. Start at the goal
state, and work backwards using the parent pointer. Repeat
until arrived back at initial cell.
</p>
<p>This is analogous to how path was reconstructed in DFS or
Dijkstra's algorithm.
</p>
<p><code>reconstruct_path</code>: implementation uses recursion, which
makes it go backward for us.
</p>
<p>Types things in <code>string_compare</code> not yet defined (four
categories): table initialization, penalty costs, goal
cell identification, traceback actions.
</p>

<h2>Chapter 11: NP-Completeness</h2>


<h2>Chapter 12: Dealing with Hard Problems</h2>


<h2>Chapter 13: How to Design Algorithms</h2>


<h2>Messages</h2>

<p>Anything tagged with <code>TADM</code> will show up here.
<b>[e6f588d8] 2022-05-05-06-17</b>: I have this feeling that the string matching algorithm stuff could make for a good game mechanic.
</p>
<p><b>[b51a22b1] 2022-05-05-06-11</b>: at this point, I feel like algorithms are getting far too "clever" for my puny brain. I will need to spend some time actually studying these carefully instead of doing these fly-by readings.
</p>
<p><b>[26ef41d4] 2022-05-05-06-02</b>: in 10.2.2, work out by hand "thou shalt" into "you should" in 5 moves as seein in figure 10.5
</p>
<p><b>[870a4820] 2022-05-04-06-19</b>: take a quick peek again at <code>string_compare_r</code> on page 316
</p>
<p><b>[5ada6c19] 2022-05-04-06-17</b>: "This program is absolutely correct - convince yourself." Story of my life with studying algorithms. Sigh.
</p>
<p><b>[845e55ec] 2022-05-04-06-01</b>: study <code>binomial_coefficient</code> code in 10.1.4, pg 313
</p>
<p><b>[6a1d5c53] 2022-05-04-06-00</b>: I don't think I know what a binomial coefficient is. May want to look that one up on Khan Academy or Wikipedia.
</p>
<p><b>[3a5d7918] 2022-05-03-06-07</b>: TIL that fibonacci numbers were invented in the thirteenth century to model the growth of rabbit populations.
</p>
<p><b>[f1abbcf4] 2022-05-03-05-54</b>: attempting chapter 10 now: dynamic programming. this concept I <b>have</b> heard of, but barely. I definitely have trouble wrapping my head around it.
</p>
<p><b>[ab1efda3] 2022-05-03-05-53</b>: just finished reading chapter 9. retention of information wasn't great. combinatorial search was a completely new concept for me. will definitely need to reread at some point.
</p>
<p><b>[3e9d44b2] 2022-05-03-05-50</b>: reread 9.7 on A* heuristic. see if it makes any more sense the second time through.
</p>
<p><b>[8833a48d] 2022-05-01-06-24</b>: 9.2 in general was very heady stuff. all worth looking at again.
</p>
<p><b>[ec13aebc] 2022-05-01-06-08</b>: study code logic in 9.2.1
</p>
<p><b>[439e04f5] 2022-04-30-06-06</b>: Maybe I'm a dumb boring reader, but I really like it when the first sentence of a chapter defines what the chapter is about. The (<a href="/brain/rustbook">rustbook</a>) did this and it was very helpful. Chapter 9 kinda rambles towards something vaguely defined as "combinatorics". I think.
</p>
<p><b>[b5dc759b] 2022-04-30-05-54</b>: as I've said before, these chapters are getting quite advanced. Even retention of broad strokes concepts is getting to be difficult. Tempted again to try out some problem sets at some point.
</p>
<p><b>[8b1555df] 2022-04-30-05-52</b>: finished reading chapter 8, onto chapter 9.
</p>
<p><b>[ba768c09] 2022-04-29-06-04</b>: The paragraph in 8.6 (randomized mincut) talking about a contraction operation has a strangly organized structure. It seems almost intentionally out of order. When I summarized it, I had to move things around to make it more clear. Always follow predictable balanced structure for technical writing.
</p>
<p><b>[13d67281] 2022-04-27-09-19</b>: look up Viterbi algorithm, which is apparently used for speech and handwriting recognition systems
</p>
<p><b>[b7daacc3] 2022-04-27-09-07</b>: floyd's algorithm I've only glossed over. I do not comprehend.
</p>
<p><b>[90ca9ac8] 2022-04-27-06-09</b>: well, they don't say cache friendly. more like very tight loops.
</p>
<p><b>[2fc4fb78] 2022-04-27-06-08</b>: floyd's algorithm: an example where big O analysis doesn't accurately report the real-world performance. Even though the time is O(n^3), the implementation itself is very cache friendly.
</p>
<p><b>[dad3d58a] 2022-04-25-06-24</b>: revisit prim's algorithm proof by contradiction blurb, page 245
</p>
<p><b>[1eeb025a] 2022-04-25-06-18</b>: Minimum spanning tree is about connecting all vertices with the least amount of effort. Does a minimum spanning tree always exist? Does a spanning tree always exist?
</p>
<p><b>[fd68d03e] 2022-04-24-14-33</b>: oof. I just finished reading chapter 7, and it feels like I barely retained anything. Many concepts I shall have to revisit. Will push onwards to chapter 8.
</p>
<p><b>[24d350e5] 2022-04-24-14-20</b>: I would say that graforge in sndkit works by explicitely defining a graph in topological order (is that how you'd phrase it?)
</p>
<p><b>[b7aee334] 2022-04-24-14-11</b>: oh wow. topological sort is the thing I've been looking for all this time for making audio graphs!
</p>
<p><b>[62b80ae3] 2022-04-24-14-03</b>: it's interesting that at the end of 7.10 on DFS on directed graphs, the author basically says "just stare at it until it makes sense". Makes me feel better about not understanding this immediately.
</p>
<p><b>[d66889ad] 2022-04-24-06-23</b>: revisit 7.9
</p>
<p><b>[077df067] 2022-04-24-06-23</b>: wow section 7.9 is over my head. will need to return to this.
</p>
<p><b>[8ee75cd0] 2022-04-24-06-16</b>: I believe I am nearly there understanding the three different kinds of articulation vertices (root, bridge, parent). But, I don't have an intuition for why. May need to let these soak a little bit.
</p>
<p><b>[43b532f1] 2022-04-24-06-15</b>: My brain is having difficulty understanding and visualizing articulation vertex detection algorithms in graphs.
</p>
<p><b>[82ea3f51] 2022-04-23-06-28</b>: "Also, bipartite graphs require distinct and categorical attributes, so they don't model the real-world variation in sexual preferences and gender identity.". I guess this is a good concluding sentence. But idk. It's a very sensitive topic for an example in an algorithms textbook. I probably would have just gone with something else to avoid the heat.
</p>
<p><b>[51fcafe1] 2022-04-23-06-15</b>: 'consider the "mutually-interested-in" graph in a hetereosexual world, where people consider only those of opposing gender'. Some of the examples in this book are a little weird. I guess it's just weird to see sexuality brought up in an algorithm textbook? and it has happened at least three times already now.
</p>
<p><b>[a30e2848] 2022-04-21-06-21</b>: I'm pretty sure this would need to be labelled as well for audio. Inputs and output cables (vertices?) need to be explicitely known.
</p>
<p><b>[2c36dcc4] 2022-04-21-06-20</b>: I could imagine a DAG structured, combined with a mechanism similar to Rust lifetimes, could produce a system that makes graforge style patches from a graph.
</p>
<p><b>[69926d9f] 2022-04-21-06-17</b>: it's really satisfying for me to read about how graphs are actually supposed to be constructed.
</p>
<p><b>[91dfe533] 2022-04-20-17-26</b>: finished reading chapter 6. now onto chapter 7.
</p>
<p><b>[2ff50063] 2022-04-18-15-54</b>: I do not know stats.
</p>
<p><b>[c47fdd55] 2022-04-18-15-31</b>: "sometimes it is more convenient to talk about the variance than standard deviation, because the term is ten characters shorter."
</p>
<p><b>[dfd4b424] 2022-04-18-06-08</b>: done with initial read of chapter 5. onto chapter 6.
</p>
<p><b>[45a40431] 2022-04-18-06-02</b>: I really don't have a malleable understanding of convolution.
</p>
<p><b>[60e025c0] 2022-04-18-06-01</b>: You can use convolution to solve a string matching algorithm in O(n*log(n)) time? wow!
</p>
<p><b>[8c16f58a] 2022-04-18-06-00</b>: it's weird to see a book talking about convolution in a context that <b>isn't</b> about reverb.
</p>
<p><b>[06db60b1] 2022-04-17-12-01</b>: the so-called "master theorem" seems like one of those things that may take a while to grok.
</p>
<p><b>[1fdece79] 2022-04-17-12-00</b>: section 5.4 and 5.5 are starting to get very theory oriented. May need to read these more slowly and give my head time to let the concepts sink in.
</p>
<p><b>[9ea0a3ab] 2022-04-16-06-31</b>: implement the counting occurences algorithm in 5.1.1 to better grok how it works.
</p>
<p><b>[ff51bc41] 2022-04-16-06-31</b>: seems to be finding boundaries of a particular key? this would be interesting to work out.
</p>
<p><b>[f2470069] 2022-04-16-06-28</b>: 5.1.1 counting occurences is making me stop and think.
</p>
<p><b>[8becadc7] 2022-04-16-06-14</b>: completing reading chapter 4. onto chapter 5.
</p>
<p><b>[d7794a21] 2022-04-15-14-21</b>: in the merge part of the mergesort implementation, the end part of it empties out buffer1 and appends before buffer2. It's not obvious to me why that works.
</p>
<p><b>[3d71ae21] 2022-04-15-14-13</b>: how does one master a particular algorithm? do people manage to derive something like mergesort from memory?
</p>
<p><b>[49ea9442] 2022-04-15-06-31</b>: try to better grok war story in section 4.4 on cheap plane tickets
</p>
<p><b>[0f46f67f] 2022-04-15-06-30</b>: I'm not fully grokking the war story in 4.4. I don't think I fully understand the problem, and I don't think I fully understand how the priority queue is being used to solve the problem.
</p>
<p><b>[0c02ecfb] 2022-04-15-05-53</b>: faster heap construction is a little too clever for me this morning.
</p>
<p><b>[b46948f9] 2022-04-14-15-18</b>: look at heapsort more closely
</p>
<p><b>[1d7c8cc8] 2022-04-14-15-18</b>: I want to look at the heapsort code more closely at some point.
</p>
<p><b>[bc3c0bef] 2022-04-14-15-14</b>: author writes return functions in C as <code>return(1)</code>. Very oldschool.
</p>
<p><b>[89816c3a] 2022-04-14-06-02</b>: onto chapter 4. I'm skipping problem sets again. naughty naughty.
</p>
<p><b>[693140f3] 2022-04-14-06-01</b>: Knuth has a bit on optimal hash table performance. Looks like Knuth 1998? Not sure what that is, I'll have to check later.
</p>
<p><b>[31582e91] 2022-04-14-05-57</b>: rote memorization doesn't seem like a terrible strategy at times with this stuff.
</p>
<p><b>[5778a069] 2022-04-14-05-57</b>: I'm noticing a lot of references to the second section which is a collection of various algorithms. making me think about taking a look.
</p>
<p><b>[9e4e7a31] 2022-04-13-14-22</b>: kinda awkward to read about how dating is an example of a priority queue.
</p>
<p><b>[741385c6] 2022-04-12-14-44</b>: in-order traversal will print items in (sorted) order. I don't know why that never clicked with me before.
</p>
<p><b>[7d20d9c8] 2022-04-11-09-43</b>: onto chapter 3: data structures
</p>
<p><b>[ae272681] 2022-04-11-09-42</b>: Interesting. This book links to leetcode problems.
</p>
<p><b>[f21c3748] 2022-04-11-09-42</b>: further reading at the end of chapter 2 may be worth checking out. One of the books, concrete mathematics, has Knuth as a co-author. So of course that interests me for that reason alone.
</p>
<p><b>[e1eea72e] 2022-04-07-13-03</b>: "a computer scientist is a mathematician who only knows how to prove things by induction".
</p>
<p><b>[d5b84960] 2022-04-06-09-44</b>: Ah, it seems this author wants you to think about the problem, not necessarily give the right answer. The author "thinks" for you in these first examples and goes through various possibilities.
</p>
<p><b>[718ee63f] 2022-04-06-09-40</b>: As I'm reading the first chapter, I'm realizing I really don't know how to think through problems involving algorithms. Can this skill be learned through practice and trial and error? Does one study how experts think about it and emulate it?
</p>
<p><b>[894f89f4] 2022-04-05-16-15</b>: just taking a look at this book now. This book is divided into two parts, with the first part being general instruction on algorithm design, and the second part being more of a reference for common algorithms. My focus will be on the first part, which is about 430 pages.
</p>
</div>
</body>
</html>
