<!DOCTYPE html>
<html lang="en">
<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="/brain/css/style.css">

</head>
<body>
<div id="main">
<title>The Algorithm Design Manual</title>
<h1>The Algorithm Design Manual</h1>
<p>By Steven S. Skiena (Third Edition)
</p>
<p>UUID: <code>gipejljdq-eoqa-heio-wauq-idrilewkhhel</code></p>
<p>Recommended book in <a href="/brain/TYCS">TYCS</a>.
</p>
<p>it is divided into two parts, "techniques" and "resources".
The former is a general introduction to algorithm design,
while the latter is inteded for browsing and reference.
</p>

<h2>Chapter 1: Introduction To Algorithm Design</h2>

<p>An algorithm is a procedure to accomplish a specific task.
</p>
<p>An algorithm must solve a generalizable set of problems. An
algorithm will perform on an <code>instance</code> of a problem.
</p>
<p>Examples: Robot Tour Optimization (The Traveling Salesman
problem) and Selecting the Right Jobs.
</p>
<p>Reasonable looking algorithms can be easily incorrect.
Algorithm <code>correctness</code> is a property that must be
carefully demonstrated.
</p>
<p>Proofs are a tool used to distinguish correct algorithms
from incorrect ones. Formal proofs will not be emphasized
in this book.
</p>
<p>Correctness of an algorithm requires that the problem
be carefully and clearly stated. Problem specifications
have two parts: the set of allowed instances, and the
require properties of the algorithm's output.
</p>
<p>An important technique in algorithm design is to narrow
the scope of allowable instances until there is an
efficient solution.
</p>
<p>Avoid ill defined quetions, such as "best" routes (what
does best mean?). Also avoid compound goals.
</p>
<p>Three common forms of algorithmic notation: English,
pseudocode, real programming language.
</p>
<p>Pseudocode: best defined as a programming language that
never complains about syntax errors.
</p>
<p>The heart of an algorithm is an idea. If your idea
is not clearly revealed when you express an algorithm,
then you are using too low-level a notation to describe it.
</p>
<p>Counterexample: an instance for an algorithm which yields
an incorrect answer.
</p>
<p>Counterexample properties: verifiability and simplicity.
</p>
<p>Verifiability: demonstrate that it is a counterexample.
Calculate what the answer would have been, and display
a better answer to prove that the algorithm didn't find it.
</p>
<p>Simplicity: strips away all details, and clearly shows
why the algorithm fails.
</p>
<p>Strategies for finding counterexamples: think small,
think exhaustively, hunt for the weakness, go for a tie,
seek extremes.
</p>
<p>Searching for counterexamples is the best way to
disprove the correctness of a heuristic.
</p>
<p>Failure to find a counterexample to a given algorithm
does it mean "it is obvious" the algorithm is correct.
A proof or demonstration of correctness is needed, often
with the use of mathematical induction.
</p>
<p>Mathematicl induction is usually the right way to
verify the correctness of a recursive or incremental
insertion algorithm.
</p>
<p>Modeling: the art of formulating your application in
terms of precisely described, well understood problems.
</p>
<p>Common structures: permutations, subsets, trees, graphs,
points, polygons, strings.
</p>
<p>Permutations: likely the object in question for problems
that seek "tour, "ordering", or "sequence".
</p>
<p>Subset: likely needed when encountering problems that
seeks a "cluster", "collection", "committee", "group",
"packaging", or "selection".
</p>
<p>Trees: likely object in question when a problem seeks
"hierarchy", "dominance relationship",
"ancestor/descendant relationship", or "taxonomy".
</p>
<p>Graphs: "network", "circuit", "web", or "relationship".
</p>
<p>Points: "sites", "position", "data records", or "locations".
</p>
<p>Polygons: "shapes", "regions", "configurations",
"boundaries".
</p>
<p>Strings: "text", "characters", "patterns", or "labels".
</p>
<p>Modeling your application in terms of well defined
structures and algorithms is the most important single step
towards a solution.
</p>
<p>Thinking recursively: looking for big things that are made
up of smaller things of exactly the same type as the big
thing.
</p>
<p>Proof by Contradiction: assume the hypothesis is false,
develop some logical consequences of assumption, show
that one consequence is false.
</p>
<p>Estimation is principled guessing. Try to solve the problem
in different ways and see if the answers generally agree
in magnitude.
</p>

<h2>Chapter 2: Algorithm Analysis</h2>

<p>Studying algorithm efficiency without implementing them.
</p>
<p>Two tools for analysis: the RAM model of computation and
asymptotic analysis of computational complexity.
</p>
<p>RAM: Random Access Machine. Hypothetical computer machine
where operation time is measured in steps.
</p>
<p>In RAM, the quality of an algorithm is determined by how it
works over all possible instances.
</p>
<p>Best case, worst case, average case.
</p>
<p>Best case: minimum number of steps taken when instance
is size N.
</p>
<p>Worst case: maximum number of steps when instance is size
N.
</p>
<p>Average case: average number of steps when instance is
size N. AKA expected time.
</p>
<p>Worse Case tends to be the most useful in practice.
</p>
<p>Average Case is helpful when working with randomized
algorithms.
</p>
<p>Big-oh notation: simplifies analysis. More "big picture".
</p>
<p>Upper bound, lower bound, something inbetween bound (not
sure what to call this)?
</p>
<p>Big oh ignores difference between multiplicative constants.
</p>
<p>Dominance: faster growing functions dominate slower
growing ones.
</p>
<p>Functions are grouped by different classes, in descending
order: Factorial, Exponential, Cubic, Quadratic,
Superlinear, Linear, Logarithmic, Constant.
</p>
<p>Special considerations for thinking about adding/multipying
functions with Big Oh. (See book).
</p>
<p>Analysis for selection sort (See book for details). Proving
two ways that the Big Oh running time is quadratic (n^2).
</p>
<p>Analysis for insertion sort. Using a "round it up" approach
to find the upper bound worst-case Big Oh time. Useful
approximation for simple algorithm analysis.
</p>
<p>Finding a substring. Analyzes each looping component
of the algorithm, and using Big Oh rules to get
an expression interms of n and m.
</p>
<p>"proving the theta": proving that the time assessment is
correct.
</p>
<p>Matrix Multiplication. Smells like cubic time, but can
be precisely proven to be. Other algorithms can do it
a bit faster.
</p>
<p>There are two basic classes of summation formulae: sum
of a power of integers, and sum of a geometric progression.
</p>
<p>Logarithms arise in problems where things are repeatedly
halved.
</p>
<p>Binary search work in logarithmic time.
</p>
<p>Range of binary values  double when you add a bit.
</p>
<p>Logarithms used to be used to multiple very large
numbers by hand.
</p>
<p>End of the chapter here covers advanced analysis
techniques. Not used in the rest of the book.
</p>

<h2>Chapter 3: Data Structures</h2>


<h3>3.1 Contiguous Vs Linked Data structures</h3>

<p>Data structures can be classified as linked or contiguous.
</p>
<p>Contiguous: structures composed of a single slab of memory.
</p>
<p>Linked: structures composed of distinct chunks of memory
tied together using pointers.
</p>
<p>Array: fundamental contiguous data struture, consisting
of a fixed size data record where each element can be
addressed using an index.
</p>
<p>Contiguous array advantages: constant-time access, space
efficiency, memory locality.
</p>
<p>Dynamic Arrays: arrays that can efficiently expand. They
double in size every time more space is needed, and items
are copied over to the new chunk. Insertion takes constant
time worst case.
</p>
<p>Linked structures advantages: overflow never happens
unless memory is actually full, insert/delete are simpler
than static arrays, moving is easier with larger records.
</p>
<p>Dynamic memory allocation provides flexibility on how/where
limited storage resources are used.
</p>

<h3>3.2 Containers, Stacks and Queues</h3>

<p>Container: abstract data type that permits
storage/retrieval of items independent of context.
</p>
<p>Containers include stacks and queues.
</p>
<p>Stack: LIFO. Simple and efficient. Good to use when
retrieval doesn't matter. Push and pop.
</p>
<p>Queue: FIFO. Trickier to implement than stacks. Good
for applications where order is important. Minimizes
that maximum time spent waiting. Enqueue and Dequeue.
</p>
<p>Stacks/Queues can be implemented using either an array
or linked list.
</p>

<h3>3.3 Dictionaries</h3>

<p>Dictionary: data type that permits access to data items
by content.
</p>
<p>Dictionary operations: search, insert, delete.
</p>
<p>Other Dictionary operations: Max/Min: retrieve item
with largest or smallest key. Predecessor/Successor:
retrieve item that comes before or after given key in
sorted order.
</p>
<p>Data structure design must balance the operations it
supports. The fastest structure supporting two
operations may not be as fast as ones that support
them individually.
</p>
<p>Comparing times for sorted or unsorted array operations.
</p>
<p>Comparing times for sorted or unsorted and
doubly or singly linked list operations.
</p>

<h3>3.4 Binary Search Trees</h3>

<p>Rooted Binary Tree: recursively defined as being empty,
or consiting of a a node called the root, together
with two rooted binary trees called left and right.
</p>
<p>Binary Tree Search: runs in O(h) time, where <code>h</code> denotes
height of tree.
</p>
<p>Binary Tree Operations (with sample code): Search, minimum,
maximum, traversal, insertion, deletion.
</p>
<p>Binary Tree Insertion: exactly one place to insert an item
X into a tree T. Can be done recursively.
</p>
<p>Binary Tree Deletion: tricky because the two descendents
have to be linked into the tree somewhere else. Three
possibilities: leaf node, node with 1 child, node with
2 children. Deleting a node with 2 children is the most
difficult to handle correctly.
</p>
<p>Performance of binary tree on average has theta log(N),
assuming all possibilities are equally probable.
</p>
<p>Balanced Search Trees: Trees that adjust after each
insertion, guaranteeing that the height is always
O(log N).
</p>
<p>The key to binary search trees is exploiting them as
black boxes.
</p>

<h3>3.6 Priority Queues</h3>

<p>Priority Queue: an abstract data type that allows
elements to be added at arbitrary times and be sorted
properly.
</p>
<p>Three Primary Operations: Insert, Find-Minimum/Find-Maximum,
Delete-Minimum/Delete-Maximum.
</p>
</p>

<h3>3.6 War story: triangle stripping</h3>

<p>War Story: using priority queues with a dictionary
to find small triangle strips that cover a given mesh.
</p>

<h3>3.7 Hashing</h3>

<p>Hash tables: very practical way to maintain a dictionary.
</p>
<p>Hash function: a mathematical function that maps keys
to integers.
</p>
<p>Collisions: when two keys map to the same integer value.
</p>
<p>Chaining: represents hash table as array of linked lists.
Natural way to resolve collision, but memory is allocated
for pointers, rather than ways to make the table larger
(and lookup faster).
</p>
<p>Open Addressing: maintains hash table as simple array of
elements (not buckets like chaining). If an index position
is filled, find the next available one closest. This
has better performance, but deletion is difficult.
</p>
<p>Hashing is a good method for duplicate detection.
</p>
<p>Duplicate detection applications that can use hashing:
checking for a document in a large corpus, document
plagiaraization, proof that file hasn't been changed.
</p>
<p>Hashing tricks: fundamentally about many-to-one mappings,
where the many isn't too many.
</p>
<p>Canonicalization: reducing complicated objects to a
standard form. Hash tables can be used here.
</p>
<p>Intentional collisions in hash tables work well for
pattern matching problems.
</p>
<p>Fingerprinting: hashing for compaction.
</p>

<h3>3.8 Specialized Data Structures</h3>

<p>Basic Data Structures: represent an unstructured set
of items that facilitate retrieval operations.
</p>
<p>Less well known: data structures for representing
more specialized objects: points, strings, graphs, sets.
</p>
<p>Similar idea: each of these have operations which
need to be performed on them, efficiently.
</p>

<h3>3.9 War Story: Sring 'em up</h3>

<p>Finding substrings in a genome sequence (very large)!
</p>
<p>Binary search tree: good until it wasn't.
</p>
<p>Profiled. Faster dictionary structure. Use hash table:
good until it wasn't.
</p>
<p>Use a suffix tree: worked until it didn't. Ran out
of memory.
</p>
<p>Compressed suffix tree: things work now!
</p>
<p>A single operation was isolated (dictionary string search)
that was being performed many times, and a performance
was improved by studying the algorithm and finding
suitable data structures for it.
</p>

<h2>Chapter 4: Sorting</h2>

<p>Sorting is fundamental to many algorithms, the source
of many interesting ideas in the field of algorithm
design, and is the most thorougly studied problem in CS.
</p>

<h3>4.1 Application Sorting</h3>

<p>Clever sorting: O(N Log(N)).
</p>
<p>Naive: O(N^2).
</p>
<p>Sorting is often a basic building block in many algorithms.
Having things sorted tends to make things way easier.
</p>
<p>Some problems that take advantage of sorting: Searching,
Closest pair, Element uniqueness, Finding the mode,
Selection, Convex hulls.
</p>
<p>Stop and think: finding the intersection. 3 approaches
involving different combinations of sorting. Small-set
sorting is best. However, a hash table could be used
to optimally solve this problem.
</p>
<p>When to use hashes?
</p>
<p>Searching: hashes are a great answer here.
</p>
<p>Closest Pair: hashes (defined thus far) do not help.
</p>
<p>Element Uniqueness: hashing is faster than sorting for
this problem.
</p>
<p>Finding the mode: linear time with hash.
</p>
<p>Finding the medium: does not help.
</p>
<p>Convex Hull: not really.
</p>

<h3>4.2 Pragmatics of Sorting</h3>

<p>What order do we want items sorted?
</p>
<p>Increasing vs decreasing order.
</p>
<p>Key vs entire record.
</p>
<p>Handling equal keys.
</p>
<p>Non-numerical data.
</p>
<p>An application specific comparison function is used
to help solve these issues, which allows sorting algorithms
to be studied independent of their context.
</p>
<p>Qsort: C standard library sorting quicksort function that
takes in a comparison function.
</p>

<h3>4.3 Heapsort: fast sorting with data structures</h3>

<p>While you are better off using standard sorting functions,
understanding how they work will be helpful for
understanding how other algorithms work.
</p>
<p>Selection sort: impelementation takes O(n) to find
smallest item in unsorted list, O(1) to remove the item.
Total time takes O(n^2). Using priority queue for these
operations will take performance from O(n^2) to O(nlog(n))
time. Priority queue can be made with heap or balanced
binary tree.
</p>
<p>Heapsort: really just selection sort with the right data
structure.
</p>
<p>Heaps: data structure used for efficiently supporting
PQ operations insert and extract-minimum.
</p>
<p>heap-labeled tree: binary tree such that the key of each
node dominates the keys of its children.
</p>
<p>min-heap: domination happens when parent node has small key
than children.
</p>
<p>heap: allows one to convey trees without pointers. Data
is stored as array of keys.
</p>
<p>Acquiring parent/children in heap: left child of <code>k</code> sits
as <code>2k</code>, right <code>2k + 1</code>, parent of <code>k</code> is at <code>k/2</code>.
</p>
<p>Missing internal nodes of a sparse tree still take up
space.
</p>
<p>Elements in heap must be packed as far left as possible,
leaving empty elements only at the end.
</p>
<p>Heap is not a binary search tree, so one can't efficiently
search through the tree.
</p>
<p>To make a heap: insert at leftmost open position, then
"bubble up" (swapping with parent) to assert dominance.
Each insert has an O(log(n)) time, with a total insert
of O(n*log(n)) time.
</p>
<p>Extract minimum from heap: pop of the top of the heap,
then replace with rightmost node (nth position in array).
Bubble down to ensure that the heap property is
satisfied (the value dominates its children). This bubbling
down is also known as <code>heapify</code>.
</p>

<h2>Chapter 5: Divide and Conquer</h2>


<h2>Chapter 6: Hasing and Randomized Algorithms</h2>


<h2>Chapter 7: Graph Traversal</h2>


<h2>Chapter 8: Weighted Graph Algorithms</h2>


<h2>Chapter 9: Combinatorial Search</h2>


<h2>Chapter 10: Dynamic Programming</h2>


<h2>Chapter 11: NP-Completeness</h2>


<h2>Chapter 12: Dealing with Hard Problems</h2>


<h2>Chapter 13: How to Design Algorithms</h2>


<h2>Messages</h2>

<p>Anything tagged with <code>TADM</code> will show up here.
</p>
</div>
</body>
</html>
